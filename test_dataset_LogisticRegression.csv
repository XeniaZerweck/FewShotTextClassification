,text,label
0,">     213       except Exception as error:
>     214         report_error_message(str(error))
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_phase.py]( in wrapper(*args, **kwargs)
>     203     def wrapper(*args, **kwargs):
>     204       try:
> --> 205         return func(*args, **kwargs)
>     206       except ConverterError as converter_error:
>     207         if converter_error.errors:
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py]( in convert_saved_model(**kwargs)
>     830   model_flags = build_model_flags(**kwargs)
>     831   conversion_flags = build_conversion_flags(**kwargs)
> --> 832   data = convert(
>     833       model_flags.SerializeToString(),
>     834       conversion_flags.SerializeToString(),
> 
> [/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py]( in convert(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
>     320       for error_data in _metrics_wrapper.retrieve_collected_errors():
>     321         converter_error.append_error(error_data)
> --> 322       raise converter_error
>     323 
>     324   return _run_deprecated_conversion_binary(model_flags_str,
> 
> ConverterError: <unknown>:0: error: loc(callsite(callsite(fused[""RandomUniform:"", ""random_uniform/RandomUniform""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.RandomUniform' op is neither a custom op nor a flex op
> <unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
> <unknown>:0: note: loc(callsite(callsite(fused[""RandomUniform:"", ""random_uniform/RandomUniform""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
> <unknown>:0: error: loc(callsite(callsite(fused[""Mul:"", ""random_uniform/Mul""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): 'tf.Mul' op is neither a custom op nor a flex op
> <unknown>:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
> <unknown>:0: note: loc(callsite(callsite(fused[""Mul:"", ""random_uniform/Mul""] at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]) at fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""])): Error code: ERROR_NEEDS_FLEX_OPS
> <unknown>:0: error: failed while converting: 'main': 
> Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select.",0
1,Here is the output from running that...,0
2,"% python3 --version
Python 3.9.6",0
3,Can you please provide more detailed repro steps?,2
4,"It's my first time contributing to open source, hope I'm not misunderstanding or doing it really wrong lol",0
5,"> 
> I am not certain about this.",1
6,"For instance, int4 type is another tensor type in which I am interested.",2
7,"Meanwhile, this issue and the solution that worked for you are related to complex types not being resolved by TS/VSCode so we can see what it actually means.",0
8,"Syntax highlighting means the display is very ""busy"" visually.",0
9,"As I see it, with `masked=False`, the input function behavior is expected not to depend on the layout nor indices of its inputs.",0
10,Same here from a Renater network where my speedtest gives me 840MB/S.,2
11,Iam attaching sample [gist]( explaining how it works in Tensorflow.,0
12,I suppose the current vscode behavior is actually very close behavior of what Mac OS actually used to behave.,2
13,"671         with OptionalXlaContext(compile_with_xla):
--> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)
    597       with autocast_variable.enable_auto_cast_variables(
    598           layer._compute_dtype_object):  # pylint: disable=protected-access
--> 599         ret = method(*args, **kwargs)
    600     _restore_layer_losses(original_losses)
    601     return ret

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)
    163       return wrapped_call(*args, **kwargs)
    164 
--> 165     return control_flow_util.smart_cond(
    166         training, lambda: replace_training_and_call(True),
    167         lambda: replace_training_and_call(False))

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)
    107     return control_flow_ops.cond(
    108         pred, true_fn=true_fn, false_fn=false_fn, name=name)
--> 109   return smart_module.smart_cond(
    110       pred, true_fn=true_fn, false_fn=false_fn, name=name)
    111 

~/tf2/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     52   if pred_value is not None:
     53     if pred_value:
---> 54       return true_fn()
     55     else:
     56       return false_fn()

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in <lambda>()
    164 
    165     return control_flow_util.smart_cond(
--> 166         training, lambda: replace_training_and_call(True),
    167         lambda: replace_training_and_call(False))
    168 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)
    161     def replace_training_and_call(training):
    162       set_training_arg(training, training_arg_index, args, kwargs)
--> 163       return wrapped_call(*args, **kwargs)
    164 
    165     return control_flow_util.smart_cond(

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in call_and_return_conditional_losses(*args, **kwargs)
    661   def call_and_return_conditional_losses(*args, **kwargs):
    662     """"""Returns layer (call_output, conditional losses) tuple.""""""",0
14,"Specified indices specify parameters, and these are different.",0
15,"Sorry for not following up, I didn't realize this was waiting on me.",0
16,"Here's the output of the model using (A):

~~~
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_15 (Dense)             (None, 64)                640       
_________________________________________________________________
dense_16 (Dense)             (None, 64)                4160      
_________________________________________________________________
dense_17 (Dense)             (None, 1)                 65        
=================================================================
Total params: 4,865
Trainable params: 4,865
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
8/8 [==============================] - 0s 13ms/step - loss: 573.9155 - mse: 573.9155 - val_loss: 577.1165 - val_mse: 577.1165
Epoch 2/100
8/8 [==============================] - 0s 6ms/step - loss: 535.4519 - mse: 535.4519 - val_loss: 535.2042 - val_mse: 535.2042
Epoch 3/100
8/8 [==============================] - 0s 5ms/step - loss: 492.2293 - mse: 492.2293 - val_loss: 486.6287 - val_mse: 486.6287
Epoch 4/100
8/8 [==============================] - 0s 5ms/step - loss: 441.6238 - mse: 441.6238 - val_loss: 429.2172 - val_mse: 429.2172
Epoch 5/100
8/8 [==============================] - 0s 5ms/step - loss: 384.5562 - mse: 384.5562 - val_loss: 367.9710 - val_mse: 367.9710
Epoch 6/100
8/8 [==============================] - 0s 5ms/step - loss: 324.1839 - mse: 324.1839 - val_loss: 304.0136 - val_mse: 304.0136
Epoch 7/100
8/8 [==============================] - 0s 5ms/step - loss: 261.1829 - mse: 261.1829 - val_loss: 237.4727 - val_mse: 237.4727
Epoch 8/100
8/8 [==============================] - 0s 6ms/step - loss: 199.2595 - mse: 199.2595 - val_loss: 175.9138 - val_mse: 175.9138
Epoch 9/100
8/8 [==============================] - 0s 5ms/step - loss: 143.4326 - mse: 143.4326 - val_loss: 122.1979 - val_mse: 122.1979
Epoch 10/100
8/8 [==============================] - 0s 5ms/step - loss: 97.0918 - mse: 97.0918 - val_loss: 79.8414 - val_mse: 79.8414
Epoch 11/100
8/8 [==============================] - 0s 5ms/step - loss: 61.4929 - mse: 61.4929 - val_loss: 48.1678 - val_mse: 48.1678
Epoch 12/100
8/8 [==============================] - 0s 5ms/step - loss: 38.5452 - mse: 38.5452 - val_loss: 30.5858 - val_mse: 30.5858
Epoch 13/100
8/8 [==============================] - 0s 6ms/step - loss: 26.7722 - mse: 26.7722 - val_loss: 22.1917 - val_mse: 22.1917
Epoch 14/100
8/8 [==============================] - 0s 6ms/step - loss: 22.0821 - mse: 22.0821 - val_loss: 19.3342 - val_mse: 19.3342
Epoch 15/100
8/8 [==============================] - 0s 6ms/step - loss: 20.2049 - mse: 20.2049 - val_loss: 17.5354 - val_mse: 17.5354
Epoch 16/100
8/8 [==============================] - 0s 5ms/step - loss: 19.3463 - mse: 19.3463 - val_loss: 17.0012 - val_mse: 17.0012
Epoch 17/100
8/8 [==============================] - 0s 5ms/step - loss: 18.8869 - mse: 18.8869 - val_loss: 16.9589 - val_mse: 16.9589
Epoch 18/100
8/8 [==============================] - 0s 6ms/step - loss: 18.7929 - mse: 18.7929 - val_loss: 16.5268 - val_mse: 16.5268
Epoch 19/100
8/8 [==============================] - 0s 5ms/step - loss: 18.6861 - mse: 18.6861 - val_loss: 16.2446 - val_mse: 16.2446
Epoch 20/100
8/8 [==============================] - 0s 5ms/step - loss: 18.6174 - mse: 18.6174 - val_loss: 16.1443 - val_mse: 16.1443
Epoch 21/100
8/8 [==============================] - 0s 5ms/step - loss: 17.9203 - mse: 17.9203 - val_loss: 16.8566 - val_mse: 16.8566
Epoch 22/100
8/8 [==============================] - 0s 5ms/step - loss: 18.1555 - mse: 18.1555 - val_loss: 16.3021 - val_mse: 16.3021
Epoch 23/100
8/8 [==============================] - 0s 5ms/step - loss: 18.0476 - mse: 18.0476 - val_loss: 15.5666 - val_mse: 15.5666
Epoch 24/100
8/8 [==============================] - 0s 5ms/step - loss: 17.9181 - mse: 17.9181 - val_loss: 15.6645 - val_mse: 15.6645
Epoch 25/100
8/8 [==============================] - 0s 5ms/step - loss: 18.1296 - mse: 18.1296 - val_loss: 15.3343 - val_mse: 15.3343
Epoch 26/100
8/8 [==============================] - 0s 5ms/step - loss: 17.5297 - mse: 17.5297 - val_loss: 15.3144 - val_mse: 15.3144
Epoch 27/100
8/8 [==============================] - 0s 6ms/step - loss: 17.4787 - mse: 17.4787 - val_loss: 15.4818 - val_mse: 15.4818
Epoch 28/100
8/8 [==============================] - 0s 6ms/step - loss: 17.2343 - mse: 17.2343 - val_loss: 15.5582 - val_mse: 15.5582
Epoch 29/100
8/8 [==============================] - 0s 5ms/step - loss: 17.3628 - mse: 17.3628 - val_loss: 15.0562 - val_mse: 15.0562
Epoch 30/100
8/8 [==============================] - 0s 5ms/step - loss: 17.4319 - mse: 17.4319 - val_loss: 14.8126 - val_mse: 14.8126
Epoch 31/100
8/8 [==============================] - 0s 5ms/step - loss: 16.8817 - mse: 16.8817 - val_loss: 14.8000 - val_mse: 14.8000
Epoch 32/100
8/8 [==============================] - 0s 6ms/step - loss: 17.0150 - mse: 17.0150 - val_loss: 14.7323 - val_mse: 14.7323
Epoch 33/100
8/8 [==============================] - 0s 5ms/step - loss: 17.0183 - mse: 17.0183 - val_loss: 14.7309 - val_mse: 14.7309
Epoch 34/100
8/8 [==============================] - 0s 6ms/step - loss: 16.7561 - mse: 16.7561 - val_loss: 14.6972 - val_mse: 14.6972
Epoch 35/100
8/8 [==============================] - 0s 6ms/step - loss: 16.8762 - mse: 16.8762 - val_loss: 14.9364 - val_mse: 14.9364
Epoch 36/100
8/8 [==============================] - 0s 5ms/step - loss: 16.7464 - mse: 16.7464 - val_loss: 14.4770 - val_mse: 14.4770
Epoch 37/100
8/8 [==============================] - 0s 7ms/step - loss: 16.9630 - mse: 16.9630 - val_loss: 14.8516 - val_mse: 14.8516
Epoch 38/100
8/8 [==============================] - 0s 5ms/step - loss: 16.5634 - mse: 16.5634 - val_loss: 14.3185 - val_mse: 14.3185
Epoch 39/100
8/8 [==============================] - 0s 5ms/step - loss: 16.2602 - mse: 16.2602 - val_loss: 14.5408 - val_mse: 14.5408
Epoch 40/100
8/8 [==============================] - 0s 5ms/step - loss: 16.5903 - mse: 16.5903 - val_loss: 14.7360 - val_mse: 14.7360
Epoch 41/100
8/8 [==============================] - 0s 5ms/step - loss: 16.2692 - mse: 16.2692 - val_loss: 14.1311 - val_mse: 14.1311
Epoch 42/100
8/8 [==============================] - 0s 5ms/step - loss: 16.1788 - mse: 16.1788 - val_loss: 14.3953 - val_mse: 14.3953
Epoch 43/100
8/8 [==============================] - 0s 5ms/step - loss: 16.2510 - mse: 16.2510 - val_loss: 14.2353 - val_mse: 14.2353
Epoch 44/100
8/8 [==============================] - 0s 5ms/step - loss: 16.0959 - mse: 16.0959 - val_loss: 13.9634 - val_mse: 13.9634
Epoch 45/100
8/8 [==============================] - 0s 5ms/step - loss: 16.2482 - mse: 16.2482 - val_loss: 13.9016 - val_mse: 13.9016
Epoch 46/100
8/8 [==============================] - 0s 5ms/step - loss: 15.7698 - mse: 15.7698 - val_loss: 15.3176 - val_mse: 15.3176
Epoch 47/100
8/8 [==============================] - 0s 6ms/step - loss: 16.0930 - mse: 16.0930 - val_loss: 14.1587 - val_mse: 14.1587
Epoch 48/100
8/8 [==============================] - 0s 5ms/step - loss: 16.1624 - mse: 16.1624 - val_loss: 14.0680 - val_mse: 14.0680
Epoch 49/100
8/8 [==============================] - 0s 5ms/step - loss: 15.7494 - mse: 15.7494 - val_loss: 13.9287 - val_mse: 13.9287
Epoch 50/100
8/8 [==============================] - 0s 5ms/step - loss: 15.5207 - mse: 15.5207 - val_loss: 13.7302 - val_mse: 13.7302
Epoch 51/100
8/8 [==============================] - 0s 5ms/step - loss: 15.8989 - mse: 15.8989 - val_loss: 13.7902 - val_mse: 13.7902
Epoch 52/100
8/8 [==============================] - 0s 6ms/step - loss: 15.4386 - mse: 15.4386 - val_loss: 13.6323 - val_mse: 13.6323
Epoch 53/100
8/8 [==============================] - 0s 5ms/step - loss: 15.5308 - mse: 15.5308 - val_loss: 13.7107 - val_mse: 13.7107
Epoch 54/100
8/8 [==============================] - 0s 6ms/step - loss: 15.2940 - mse: 15.2940 - val_loss: 13.5425 - val_mse: 13.5425
Epoch 55/100
8/8 [==============================] - 0s 6ms/step - loss: 15.2990 - mse: 15.2990 - val_loss: 13.6214 - val_mse: 13.6214
Epoch 56/100
8/8 [==============================] - 0s 6ms/step - loss: 15.5306 - mse: 15.5306 - val_loss: 13.7859 - val_mse: 13.7859
Epoch 57/100
8/8 [==============================] - 0s 6ms/step - loss: 15.2955 - mse: 15.2955 - val_loss: 13.4464 - val_mse: 13.4464
Epoch 58/100
8/8 [==============================] - 0s 5ms/step - loss: 15.3422 - mse: 15.3422 - val_loss: 14.4806 - val_mse: 14.4806
Epoch 59/100
8/8 [==============================] - 0s 5ms/step - loss: 15.2947 - mse: 15.2947 - val_loss: 13.3782 - val_mse: 13.3782
Epoch 60/100
8/8 [==============================] - 0s 5ms/step - loss: 15.2226 - mse: 15.2226 - val_loss: 13.6523 - val_mse: 13.6523
Epoch 61/100
8/8 [==============================] - 0s 6ms/step - loss: 14.7474 - mse: 14.7474 - val_loss: 15.0304 - val_mse: 15.0304
Epoch 62/100
8/8 [==============================] - 0s 5ms/step - loss: 14.9154 - mse: 14.9154 - val_loss: 13.6243 - val_mse: 13.6243
Epoch 63/100
8/8 [==============================] - 0s 5ms/step - loss: 15.0523 - mse: 15.0523 - val_loss: 13.1219 - val_mse: 13.1219
Epoch 64/100
8/8 [==============================] - 0s 5ms/step - loss: 15.0700 - mse: 15.0700 - val_loss: 13.0726 - val_mse: 13.0726
Epoch 65/100
8/8 [==============================] - 0s 5ms/step - loss: 15.1012 - mse: 15.1012 - val_loss: 13.1141 - val_mse: 13.1141
Epoch 66/100
8/8 [==============================] - 0s 7ms/step - loss: 15.1817 - mse: 15.1817 - val_loss: 12.9964 - val_mse: 12.9964
Epoch 67/100
8/8 [==============================] - 0s 5ms/step - loss: 14.7159 - mse: 14.7159 - val_loss: 13.1849 - val_mse: 13.1849
Epoch 68/100
8/8 [==============================] - 0s 5ms/step - loss: 15.0554 - mse: 15.0554 - val_loss: 12.9389 - val_mse: 12.9389
Epoch 69/100
8/8 [==============================] - 0s 5ms/step - loss: 14.6790 - mse: 14.6790 - val_loss: 13.2163 - val_mse: 13.2163
Epoch 70/100
8/8 [==============================] - 0s 5ms/step - loss: 14.4835 - mse: 14.4835 - val_loss: 12.9312 - val_mse: 12.9312
Epoch 71/100
8/8 [==============================] - 0s 6ms/step - loss: 14.4713 - mse: 14.4713 - val_loss: 12.9020 - val_mse: 12.9020
Epoch 72/100
8/8 [==============================] - 0s 5ms/step - loss: 14.5055 - mse: 14.5055 - val_loss: 12.8103 - val_mse: 12.8103
Epoch 73/100
8/8 [==============================] - 0s 5ms/step - loss: 14.6565 - mse: 14.6565 - val_loss: 12.7457 - val_mse: 12.7457
Epoch 74/100
8/8 [==============================] - 0s 5ms/step - loss: 14.4647 - mse: 14.4647 - val_loss: 12.7685 - val_mse: 12.7685
Epoch 75/100
8/8 [==============================] - 0s 6ms/step - loss: 14.2886 - mse: 14.2886 - val_loss: 12.7231 - val_mse: 12.7231
Epoch 76/100
8/8 [==============================] - 0s 6ms/step - loss: 14.2142 - mse: 14.2142 - val_loss: 12.6104 - val_mse: 12.6104
Epoch 77/100
8/8 [==============================] - 0s 5ms/step - loss: 14.3990 - mse: 14.3990 - val_loss: 12.9287 - val_mse: 12.9287
Epoch 78/100
8/8 [==============================] - 0s 5ms/step - loss: 14.4393 - mse: 14.4393 - val_loss: 13.0988 - val_mse: 13.0988
Epoch 79/100
8/8 [==============================] - 0s 7ms/step - loss: 14.2155 - mse: 14.2155 - val_loss: 12.4750 - val_mse: 12.4750
Epoch 80/100
8/8 [==============================] - 0s 6ms/step - loss: 14.0817 - mse: 14.0817 - val_loss: 13.0547 - val_mse: 13.0547
Epoch 81/100
8/8 [==============================] - 0s 6ms/step - loss: 14.1034 - mse: 14.1034 - val_loss: 12.5163 - val_mse: 12.5163
Epoch 82/100
8/8 [==============================] - 0s 5ms/step - loss: 13.9492 - mse: 13.9492 - val_loss: 12.4928 - val_mse: 12.4928
Epoch 83/100
8/8 [==============================] - 0s 5ms/step - loss: 14.0635 - mse: 14.0635 - val_loss: 12.6222 - val_mse: 12.6222
Epoch 84/100
8/8 [==============================] - 0s 5ms/step - loss: 13.9243 - mse: 13.9243 - val_loss: 12.3200 - val_mse: 12.3200
Epoch 85/100
8/8 [==============================] - 0s 6ms/step - loss: 14.0242 - mse: 14.0242 - val_loss: 12.2624 - val_mse: 12.2624
Epoch 86/100
8/8 [==============================] - 0s 6ms/step - loss: 13.9066 - mse: 13.9066 - val_loss: 12.3266 - val_mse: 12.3266
Epoch 87/100
8/8 [==============================] - 0s 5ms/step - loss: 13.6502 - mse: 13.6502 - val_loss: 12.2916 - val_mse: 12.2916
Epoch 88/100
8/8 [==============================] - 0s 5ms/step - loss: 14.1173 - mse: 14.1173 - val_loss: 12.7280 - val_mse: 12.7280
Epoch 89/100
8/8 [==============================] - 0s 5ms/step - loss: 13.4772 - mse: 13.4772 - val_loss: 12.1312 - val_mse: 12.1312
Epoch 90/100
8/8 [==============================] - 0s 5ms/step - loss: 13.5305 - mse: 13.5305 - val_loss: 12.6813 - val_mse: 12.6813
Epoch 91/100
8/8 [==============================] - 0s 5ms/step - loss: 13.4792 - mse: 13.4792 - val_loss: 12.7643 - val_mse: 12.7643
Epoch 92/100
8/8 [==============================] - 0s 5ms/step - loss: 12.7118 - mse: 12.7118 - val_loss: 13.1304 - val_mse: 13.1304
Epoch 93/100
8/8 [==============================] - 0s 6ms/step - loss: 13.4823 - mse: 13.4823 - val_loss: 12.6384 - val_mse: 12.6384
Epoch 94/100
8/8 [==============================] - 0s 5ms/step - loss: 13.3819 - mse: 13.3819 - val_loss: 13.0808 - val_mse: 13.0808
Epoch 95/100
8/8 [==============================] - 0s 6ms/step - loss: 13.3348 - mse: 13.3348 - val_loss: 12.1661 - val_mse: 12.1661
Epoch 96/100
8/8 [==============================] - 0s 5ms/step - loss: 13.1636 - mse: 13.1636 - val_loss: 11.8638 - val_mse: 11.8638
Epoch 97/100
8/8 [==============================] - 0s 6ms/step - loss: 13.0808 - mse: 13.0808 - val_loss: 12.3383 - val_mse: 12.3383
Epoch 98/100
8/8 [==============================] - 0s 5ms/step - loss: 13.2087 - mse: 13.2087 - val_loss: 11.7358 - val_mse: 11.7358
Epoch 99/100
8/8 [==============================] - 0s 6ms/step - loss: 13.1870 - mse: 13.1870 - val_loss: 11.9876 - val_mse: 11.9876
Epoch 100/100
8/8 [==============================] - 0s 5ms/step - loss: 13.2171 - mse: 13.2171 - val_loss: 12.1973 - val_mse: 12.1973

MSE ON TRAINING:  12.762719337215518

PREDICTIONS
>> actual=15.0, predicted=13.902673721313477
>> actual=10.0, predicted=11.325315475463867
>> actual=9.0, predicted=11.17175579071045
>> actual=25.0, predicted=25.51102638244629
>> actual=19.0, predicted=20.770906448364258
>> actual=14.0, predicted=13.679293632507324
>> actual=14.0, predicted=14.169425964355469
>> actual=13.0, predicted=13.453120231628418
>> actual=18.0, predicted=20.06334114074707
>> actual=35.0, predicted=32.093849182128906
>> actual=25.0, predicted=28.19291877746582
>> actual=19.0, predicted=26.57512092590332
>> actual=13.0, predicted=15.338750839233398
>> actual=28.0, predicted=28.23664093017578
>> actual=13.0, predicted=13.387893676757812
>> actual=14.0, predicted=15.006385803222656
>> actual=15.0, predicted=14.883852005004883
>> actual=13.0, predicted=13.88344955444336
>> actual=18.0, predicted=20.642784118652344
>> actual=12.0, predicted=12.577205657958984
MSE ON TESTING: 8.60193713820663
~~~

Here's the output of the model with (B):

~~~
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_18 (Dense)             (None, 64)                640       
_________________________________________________________________
dense_19 (Dense)             (None, 64)                4160      
_________________________________________________________________
dense_20 (Dense)             (None, 1)                 65        
=================================================================
Total params: 4,865
Trainable params: 4,865
Non-trainable params: 0
_________________________________________________________________
ep=0	loss=451.15	fit-mse=562.54	dev-mse=605.75
ep=1	loss=634.75	fit-mse=528.33	dev-mse=570.50
ep=2	loss=578.77	fit-mse=489.81	dev-mse=530.83
ep=3	loss=383.15	fit-mse=445.30	dev-mse=485.17
ep=4	loss=330.32	fit-mse=394.18	dev-mse=433.01
ep=5	loss=350.10	fit-mse=338.05	dev-mse=375.80
ep=6	loss=275.47	fit-mse=280.20	dev-mse=316.84
ep=7	loss=216.04	fit-mse=224.03	dev-mse=259.55
ep=8	loss=219.84	fit-mse=171.60	dev-mse=205.86
ep=9	loss=169.93	fit-mse=128.87	dev-mse=161.59
ep=10	loss=86.35	fit-mse=98.62	dev-mse=129.46
ep=11	loss=67.55	fit-mse=81.66	dev-mse=110.51
ep=12	loss=63.47	fit-mse=70.28	dev-mse=96.27
ep=13	loss=40.30	fit-mse=64.55	dev-mse=88.48
ep=14	loss=44.24	fit-mse=60.42	dev-mse=82.77
ep=15	loss=55.45	fit-mse=58.35	dev-mse=79.49
ep=16	loss=66.44	fit-mse=57.02	dev-mse=77.70
ep=17	loss=49.05	fit-mse=55.90	dev-mse=75.52
ep=18	loss=96.35	fit-mse=54.94	dev-mse=74.29
ep=19	loss=46.29	fit-mse=54.72	dev-mse=74.27
ep=20	loss=76.34	fit-mse=55.96	dev-mse=76.21
ep=21	loss=54.97	fit-mse=56.31	dev-mse=76.22
ep=22	loss=54.00	fit-mse=54.82	dev-mse=74.10
ep=23	loss=65.32	fit-mse=55.19	dev-mse=74.64
ep=24	loss=53.25	fit-mse=56.81	dev-mse=77.45
ep=25	loss=40.72	fit-mse=56.05	dev-mse=75.60
ep=26	loss=46.65	fit-mse=55.25	dev-mse=74.81
ep=27	loss=57.01	fit-mse=55.16	dev-mse=74.98
ep=28	loss=64.03	fit-mse=55.09	dev-mse=74.56
ep=29	loss=68.08	fit-mse=55.22	dev-mse=75.21
ep=30	loss=69.12	fit-mse=53.94	dev-mse=73.48
ep=31	loss=42.17	fit-mse=55.08	dev-mse=75.19
ep=32	loss=39.22	fit-mse=56.67	dev-mse=77.53
ep=33	loss=76.51	fit-mse=56.36	dev-mse=76.03
ep=34	loss=70.53	fit-mse=55.41	dev-mse=75.23
ep=35	loss=36.58	fit-mse=54.01	dev-mse=73.56
ep=36	loss=65.90	fit-mse=53.55	dev-mse=72.95
ep=37	loss=59.30	fit-mse=53.91	dev-mse=73.55
ep=38	loss=61.93	fit-mse=54.09	dev-mse=73.31
ep=39	loss=66.33	fit-mse=54.25	dev-mse=73.54
ep=40	loss=72.43	fit-mse=55.24	dev-mse=75.45
ep=41	loss=37.08	fit-mse=55.69	dev-mse=76.03
ep=42	loss=47.11	fit-mse=55.91	dev-mse=75.43
ep=43	loss=42.63	fit-mse=54.87	dev-mse=75.01
ep=44	loss=54.36	fit-mse=55.95	dev-mse=76.42
ep=45	loss=26.99	fit-mse=55.46	dev-mse=75.88
ep=46	loss=76.64	fit-mse=55.95	dev-mse=75.45
ep=47	loss=48.25	fit-mse=56.55	dev-mse=76.82
ep=48	loss=54.42	fit-mse=55.08	dev-mse=74.42
ep=49	loss=73.48	fit-mse=55.05	dev-mse=74.62
ep=50	loss=43.22	fit-mse=55.49	dev-mse=75.26
ep=51	loss=54.03	fit-mse=54.87	dev-mse=74.38
ep=52	loss=66.74	fit-mse=54.54	dev-mse=73.11
ep=53	loss=65.90	fit-mse=55.99	dev-mse=75.91
ep=54	loss=58.84	fit-mse=56.37	dev-mse=75.97
ep=55	loss=49.63	fit-mse=55.29	dev-mse=74.15
ep=56	loss=48.78	fit-mse=55.60	dev-mse=74.83
ep=57	loss=54.61	fit-mse=54.00	dev-mse=72.63
ep=58	loss=47.21	fit-mse=55.10	dev-mse=74.48
ep=59	loss=80.70	fit-mse=54.28	dev-mse=72.64
ep=60	loss=66.65	fit-mse=55.22	dev-mse=74.33
ep=61	loss=82.05	fit-mse=53.16	dev-mse=70.96
ep=62	loss=51.54	fit-mse=54.64	dev-mse=73.51
ep=63	loss=48.69	fit-mse=56.23	dev-mse=76.05
ep=64	loss=92.59	fit-mse=55.95	dev-mse=74.36
ep=65	loss=62.16	fit-mse=55.92	dev-mse=75.03
ep=66	loss=46.01	fit-mse=55.84	dev-mse=75.34
ep=67	loss=50.22	fit-mse=56.25	dev-mse=76.24
ep=68	loss=92.09	fit-mse=55.56	dev-mse=73.48
ep=69	loss=53.25	fit-mse=55.99	dev-mse=75.80
ep=70	loss=71.81	fit-mse=55.77	dev-mse=75.19
ep=71	loss=66.39	fit-mse=55.28	dev-mse=74.20
ep=72	loss=54.23	fit-mse=54.80	dev-mse=74.16
ep=73	loss=54.83	fit-mse=55.40	dev-mse=74.64
ep=74	loss=40.68	fit-mse=55.03	dev-mse=73.95
ep=75	loss=81.32	fit-mse=54.96	dev-mse=73.06
ep=76	loss=50.36	fit-mse=55.94	dev-mse=75.51
ep=77	loss=64.00	fit-mse=56.08	dev-mse=75.65
ep=78	loss=70.95	fit-mse=54.84	dev-mse=74.51
ep=79	loss=64.96	fit-mse=53.87	dev-mse=72.72
ep=80	loss=56.27	fit-mse=54.93	dev-mse=74.10
ep=81	loss=61.17	fit-mse=55.11	dev-mse=74.21
ep=82	loss=60.23	fit-mse=57.36	dev-mse=77.07
ep=83	loss=41.64	fit-mse=55.80	dev-mse=75.71
ep=84	loss=38.13	fit-mse=55.96	dev-mse=75.40
ep=85	loss=58.91	fit-mse=57.40	dev-mse=77.60
ep=86	loss=47.32	fit-mse=56.12	dev-mse=75.42
ep=87	loss=77.51	fit-mse=55.13	dev-mse=74.06
ep=88	loss=75.37	fit-mse=53.90	dev-mse=72.67
ep=89	loss=49.26	fit-mse=55.25	dev-mse=74.08
ep=90	loss=91.33	fit-mse=52.48	dev-mse=70.72
ep=91	loss=38.65	fit-mse=52.77	dev-mse=71.15
ep=92	loss=43.26	fit-mse=54.84	dev-mse=73.87
ep=93	loss=59.81	fit-mse=55.12	dev-mse=73.99
ep=94	loss=60.65	fit-mse=54.58	dev-mse=73.89
ep=95	loss=49.32	fit-mse=54.81	dev-mse=73.37
ep=96	loss=37.07	fit-mse=55.97	dev-mse=75.03
ep=97	loss=51.49	fit-mse=55.03	dev-mse=73.78
ep=98	loss=32.92	fit-mse=55.53	dev-mse=75.13
ep=99	loss=79.23	fit-mse=54.59	dev-mse=72.64

MSE ON TRAINING:  58.21192624872478

PREDICTIONS
>> actual=15.0, predicted=22.958749771118164
>> actual=10.0, predicted=23.622652053833008
>> actual=9.0, predicted=23.67362403869629
>> actual=25.0, predicted=23.90199851989746
>> actual=19.0, predicted=23.206247329711914
>> actual=14.0, predicted=23.894834518432617
>> actual=14.0, predicted=23.627328872680664
>> actual=13.0, predicted=24.387636184692383
>> actual=18.0, predicted=23.90337562561035
>> actual=35.0, predicted=24.2421932220459
>> actual=25.0, predicted=24.03383445739746
>> actual=19.0, predicted=24.37618064880371
>> actual=13.0, predicted=23.93202781677246
>> actual=28.0, predicted=24.228940963745117
>> actual=13.0, predicted=23.033733367919922
>> actual=14.0, predicted=23.603410720825195
>> actual=15.0, predicted=23.061141967773438
>> actual=13.0, predicted=23.7595157623291
>> actual=18.0, predicted=23.451452255249023
>> actual=12.0, predicted=23.622989654541016
MSE ON TESTING: 63.039009816852854
~~~

**Describe the expected behavior**

The output of these models should be similar, or at least within the same ballpark.",0
17,Closing as stale.,0
18,"Please confirm the TF version you are using and correct it in template.You can find tested configurations of cuda,cudnn wrt TF version etc from [here](

Once ensured Cuda driver installation then we should go for cuda toolkit installation and path setting.",2
19,Unfortunately I wasn't able to repro this.,2
20,One reason for this is the easier transition back to legacy systems.,0
21,What I'm trying to communicate is that each pytorch op then needs a way to know which argument sets the device for the rest of the arguments of that op.,0
22,E.g we should know that files have been opened and we should be asking for info.,0
23,"> -israyelyan In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here.",2
24,It may though require a full svd approach or some custom Newton-step approximation (saw it in some paper for a fast matrix square root estimation without doing an svd),0
25,"There is nothing to fix in backward, it is correct.",0
26,My bad.,0
27,For usage in LLM it would defo be super useful.,0
28,"So VS Code is not in a position to introduce a ""terminal group"" property in all launch configs.",0
29,"### 🐛 Describe the bug
Something like this:

### Versions
main
cc",0
30,"However, there are no tests to verify it is correct.",1
31,Thanks for the replies.,0
32,"-israyelyan 
Sorry that I didn't confirmed it with limited resources on colab.",0
33,"Hi , The **CORRECT FIX** is a true VSCode Setting from the team.",0
34,<br> If you are unable to remove the `Stale` label please contact a maintainer in order to do so.,0
35,Hey!,0
36,not sure why this wouldn't work for you and cannot repro so can't investigate.,1
37,"The build_tf_flex.sh file is a customized file I made by taking the build_aar.sh file from tensorflow source code and modifying it to:

      - Build the tensorflowlite.so with the command: 
      bazel ${CACHE_DIR_FLAG} build -c opt --cxxopt='--std=c++17' --config=${TARGET_ARCHS} //tmp:tensorflowlite --verbose_failures
      
      - Build the tensorflowlite_flex.so with the command:
      bazel ${CACHE_DIR_FLAG} build -c opt --cxxopt='--std=c++17' --config=${TARGET_ARCHS} --host_crosstool_top=//tools/cpp:toolchain //tmp:tensorflowlite_flex --verbose_failures

The tensorflowlite.so file is successfully created, but at the tensorflowlite_flex.so building step I obtain the mentioned error:
`fatal error: dnnl.hpp: No such file or directory`

This is the log file: 
[error_logs](",0
38,"Left is version in git, right is local version.",0
39,This function is not differentiable since it's right derivative is not finite which the example from above shows.,0
40,"Both could be able to leverage a common configuration so that whether a terminal leveraging such a theoretical common component is spawned within VScode or standalone, they behave the same.",0
41,But is the expectation to have xla_hlo.const and I think there is a conversion from xla_hlo.const to xla_lhlo.const.,0
42,"It should work just like stepping in an editor, which I think it does today.",2
43,"A simplified explanation would be with the usual drawing of the deep nn:


Then it depends on the what parts are being used in each block, if they are not shared, there is no need to copy ever, other than at the boundaries.",0
44,"Not using up all the memory at once sounds like a useful feature, however I am looking to clear the memory tf has already taken.",2
45,"Can you please update your code based on the [gist]( provided in [this issue](

Need to update and reset states as mentioned in that issue.",2
46,"I use the most recent canary and for me it becomes really slow and buggy (disappearing texts, frozen windows and so on) as well, while the terminal in VS Code seems to work really good in everything I tried so far.",0
47,"I guess `(3, 1, 62)` ?",1
48,"> We would need to check if it works on Windows too where we resort to ""named pipes"".",1
49,"-->
- VS Code Version: 1.74.3
- OS Version: Windows_NT x64 10.0.19045

Steps to Reproduce:

1. bind `workbench.action.togglePanel` command to button
2. make sure terminal has scrollbar
3. toggle panel on/off and see that scrollbar disappears",0
50,please help.,0
51,"The most useful features that come to mind are:

1.",0
52,"Testing #20793 

I would have expected to see an inline instruction pointer decoration when I use the 'Run to Cursor' command.

!",2
53,"That is it's not enough to move (1) the model and (2) the data `.to(device)`, we have to (3) explicitly change the default device too.",0
54,"Actually, I stand corrected on self observation, so my apologies on the CuDNN version.",0
55,"That is, the expectation I'd have for this is in the short-medium term is everything will be the same as your writeup except you don't have to do source hacking of the actual operator definitions.",2
56,"Long story short, I could not just move the entire search model over to `/browser/` (maybe you would want to do that since that only executes in a browser environment), so the result is the `RangeHighlightDecorations` from `searchModel.ts` which works with a text model (`IModelService`) and not with an editor (`IEditorService`).",0
57,"This is still broken as of


I repro'd by opening an Unreal Engine workspace, searching for `FCustomPrimitiveData` and stepping through results.",0
58,Unless you can gaurantee that the input is going to always have `xla_hlo.const` instead of `std.constant` you need a way to convert `std.constant` to `xla_lhlo.const` (similar to how `xla_hlo.const` is converted to `xla_lhlo.const`).,0
59,Behavior in 1.68.1 is the same as 1.56.2 (May 2021).,0
60,"Interestingly, the warning is only displayed if I have a masking layer with the generator of my GAN.",2
61,VScode Terminal as a stand-alone app would be great to have but I can see the architectural difficulties it may present and vscode team may be hesitant to it.,0
62,Really dont see either approaches being better than the other.,2
63,Replying to my own comment...,0
64,", can you please take a look?",2
65,#27878 is a related issue discussing context manager forcing created tensor to be on a specified device.,0
66,> Is it being applied only to differentiable inputs?,1
67,"Thanks for the warning, I am in fact using TF 2.2 with an RTX card...",0
68,**Standalone code to reproduce the issue**,0
69,"As mentioned, the model can be executed correctly on Colab.",0
70,"So the solution would be to either build the library yourself or to make sure that you get the right ABI, which I see you can now pick when downloading the C++ library.",0
71,'s example did not compile for me.,2
72,I tried this and TensorFlow is so blind it cannot see my 1030.,0
73,Is it as easy as checking for `\\` in the path beginning?,1
74,"### Alternatives
_No response_
### Additional context
_No response_
cc                 -Chen    -nrv             -w -Weiwen",0
75,I use Miniconda.,0
76,I'm interested in the thinking behind moving away from our own tokenization in favor of tmLanguages.,1
77,Being able to undock the terminal panel as it’s own maximizable window and switch from dropdown to tabs should allievate a lot of pain.,0
78,"VS Code version: Code 1.31.0 (7c66f58312b48ed8ca4e387ebd9ffe9605332caa, 2019-02-06T08:51:24.856Z)
OS version: Linux x64 4.15.0-1032-oem

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz (8 x 2874)|
|GPU Status|2d_canvas: enabled<br>checker_imaging: disabled_off<br>flash_3d: enabled<br>flash_stage3d: enabled<br>flash_stage3d_baseline: enabled<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>native_gpu_memory_buffers: disabled_software<br>rasterization: disabled_software<br>surface_synchronization: enabled_on<br>video_decode: unavailable_off<br>webgl: enabled<br>webgl2: enabled|
|Load (avg)|2, 3, 3|
|Memory (System)|15.39GB (1.47GB free)|
|Process Argv||
|Screen Reader|no|
|VM|0%|

</details><details><summary>Extensions (20)</summary>

Extension|Author (truncated)|Version
---|---|---
project-manager|ale|10.3.2
quitcontrol-vscode|art|3.0.0
better-toml|bun|0.3.2
whitespace-plus|dav|0.0.5
mustache|daw|1.1.1
gitlens|eam|9.5.0
EditorConfig|Edi|0.12.8
githd|hui|2.1.0
rpm-spec|Lau|0.2.3
vscode-duplicate|mrm|1.2.1
indent-rainbow|ode|7.2.4
vscode-subword-navigation|ow|1.2.0
vscode-docker|Pet|0.5.2
rust|rus|0.5.3
whitespace|san|0.0.5
crates|ser|0.3.6
code-settings-sync|Sha|3.2.4
local-history|xyz|1.7.0
plsql-language|xyz|1.7.0
markdown-all-in-one|yzh|2.0.1


</details>
<!-- generated by issue reporter -->",0
79,"This hacked together fix may be helpful to others with the same problem:
[notebook](",2
80,"In PyTorch I could save rng and load rng to get the same shuffling order, but in Tensorflow I don't know how to do that.",2
81,"Also, their use as loss functions can significantly enhance image quality in tasks such as image reconstruction, denoising, and super-resolution.",0
82,Please note that the app is just a modified version of the existing StyleTransfer app for the purpose of loading the model in question.,0
83,"However, what if I want to reshuffle after each iteration (epoch) and still be able to reproduce the iterator state (shuffle order) from the checkpoint?",1
84,"So my thinking is this - **the next stage is to extend pytorch ops to automatically switch the inputs to the device of the params at the point of use**, so if there is a `matmul` look at the device of the ""designated"" argument and switch all the other args to the same device, rather than assert that they have to be on the same device.",0
85,"***>
wrote:
> Testing #129161 <
>
>  < -MSFT
> <
>
> When scrolling the current selected frame out of view if you click to step
> the viewport should scroll and return to the same position.",0
86,> I would also argue that the TF legalization should produce an HLO constant so that its output is closed in HLO.,0
87,"> `gradcheck` with `masked=False` assumes that its input function `func` is layout-agnostic, that is, `func(densify(x)).to_dense() == func(x).to_dense()` holds for `x` with any layout, where `densify` is a function that materializes all unspecified elements of its input as zeros.",0
88,:(,0
89,It's very easy to add support for a new language.,0
90,Why does the above command posted only work as the root user in a Python3.9 Anaconda activated environment?,1
91,"Input 0 of layer &quot;conv2d_2&quot; is incompatible with the layer: expected axis -1 of input shape to have value 128, but received input with shape (None, 28, 28, 3)

Call arguments received by layer &quot;sequential_4&quot; (type Sequential):
  • inputs=tf.Tensor(shape=(None, 28, 28, 3), dtype=float32)
  • training=None
  • mask=None
</pre>",0
92,This would be useful to get some buy in and to show it's a compelling proposition.,0
93,Nice ~,0
94,"This comes straight from following your instructions as well as TensorFlows:

``pip3 install nvidia cudnn-cu11==8.6.0.163
ERROR: Could not find a version that satisfies the requirement nvidia (from versions: none)
ERROR: No matching distribution found for nvidia``",0
95,"The instance has this specifications:

RAM: 32GB
vCPU: 16
Architecture: x86_64
OS: Ubuntu 22.0.4 Server

I use the same simple model of my first comment, previously converted and saved to a .tflite, setting _tf.lite.OpsSet.TFLITE_BUILTINS_ and _tf.lite.OpsSet.SELECT_TF_OPS_ .",2
96,"Checking on this, a year(!)",0
97,"(Also, no one is signed up to do it, this is just the consensus for how it should be done atm.)",0
98,I thought 5 since those are the indices that can be applied as is to the tensor.,2
99,"Hi , `rhs ` is missing.Thank you",0
100,Happy Coding!,0
101,"But I reckon there will probably be some electron limitations that make this very hard to achieve, if not impossible.",1
102,"Rather, i think it is more appropriate for somebody intimately knowledgeable with tensorflow to draw a dependency graph.",0
103,"However, I had something else in mind.",0
104,"Oh, by the way, Anaconda is exactly the same as Python 3, because you can run Python 3 code in it.",0
105,We are not actively developing on this FSDP anymore (i.e.,0
106,Thank you very much for your help.,0
107,"Many disciplines, including but not limited to physics and engineering, are eagerly anticipating this feature.",0
108,I'm also seeing this warning when trying to train a custom image segmentation model with pixellib,2
109,VSCode's terminal implementation worked so good that I don't use cmd.exe anymore.,0
110,"Yeah so we iterate over each parameter in the model and clip that to
something.",0
111,(I kill the process using `Ctrl + C`.,0
112,"See instructions:  
> TF Select ops: Mul, RandomUniform
> Details:
> 	tf.Mul(tensor<600xf64>, tensor<f64>) -> (tensor<600xf64>) : {device = """"}
> 	tf.RandomUniform(tensor<1xi32>) -> (tensor<600xf64>) : {device = """", seed = 0 : i64, seed2 = 0 : i64}


I'm trying to convert this simple model to Tensorflow Lite using the _experimental_select_user_tf_ops_ flag to tell the converter what operations from the tf_ops set to include.",2
113,"What's weird is that even though I'm feeding the same size of data on both PC and mobile, this error only appears on the mobile.",1
114,"`sampled_addmm` actually falls into the hardest, `Simple` category, and the user is not aware of that AT ALL!",0
115,"tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Monterey 12.1, MacBook Pro M1 2021**
- Mobile device (e.g.",0
116,Is this not the case?,1
117,"If it's just one or two well known ones, we might as well just put them in core.",0
118,"- Leave the 32 bit implementation, or implement both a 32 and 64 bit implementation, or just the 64 bit implementation?",1
119,"Please be aware that in contrast to tasks, launch configurations do not know anything about terminals.",0
120,Ported it with compat.v1 methods till Version 2.8 it was fine.,0
121,Is this not what you are seeing?,2
122,>  Could you please provide how CUDA/CUDNN was installed on your machine ?,2
123,<br> Feel free to remove the `Stale` label if you feel this was a mistake.,0
124,"Searching user settings I cannot find anything for ""triple"".",1
125,"Does it mean that the total norm is
> only computed per gradient rather than overall all gradients of the model?",1
126,"Hmm, I don't see a reason why CPU has to be different.",0
127,"[image](

 did you configure anything in your settings that could prevent IndexedDB from being opened or created or maybe some quota limit is hit.",1
128,You should check out  also Electron based and extensible.,0
129,"### Alternatives

#105045

### Additional context
cc",0
130,"Ah alright, thanks.",0
131,I do not use Anaconda for something I can run in Python.,0
132,"Thanks , sounds good to me.",0
133,Is there any update on this?,1
134,Yeah I think it has to do with scrolling a lot in the infinite scroll,2
135,"Hi -arm, so currently the quantization is unable to quantize the unpack operator, so it actually processes that op in the original bit size.",0
136,Microsoft would be beholden to emulate rather than evolve it.,0
137,"I'm having this issue too

(When I run `code --crash-reporter-directory`, it only generates `settings.dat` and empty directories, no `.dmp` file.)",2
138,[image](,0
139,No.,0
140,"That being said, ""the quantization sometimes works already if one just tries it enough times"" doesn't sound good, is there any way you can provide us information on when it starts working?",2
141,I'm having a similar issue when creating a new file/folder or renaming a file/folder.,0
142,"Currently TF server does not support clean shutdown, and as a workaround I'd suggest you to use a dummy server to probe available ports instead of creating and destroying TF servers repeatedly.",0
143,"This PR adds a warning that `torch.svd` is deprecated, with 's instructions from 
In addition, all usage of the old svd function is replaced with a new one from `torch.linalg` module.",0
144,"great findings, I think we had this issue for a long time without understanding it fully.",1
145,- Simpler.,0
146,", if projects can be referenced as folders.",0
147,"For my future projects, I won't do the same mistake tho and I can clearly see from the papers trends that it's where everyone is heading to.",0
148,Is that version you mentioned is CUDA or cuDNN ?,1
149,"For output background color, I've actually made it transparent (`""notebook.outputContainerBackgroundColor"": ""#0000""`) in my user settings which tends to look a little better with charts + tables:

## Before
<img width=""1035"" alt=""CleanShot 2021-06-09 at 09 58 12"" src=""

<img width=""788"" alt=""CleanShot 2021-06-09 at 10 02 14"" src=""


## After
<img width=""1026"" alt=""CleanShot 2021-06-09 at 09 58 40"" src=""

<img width=""779"" alt=""CleanShot 2021-06-09 at 10 03 17"" src=""",0
150,"Yes, thank you for your reply.",0
151,"---

 Any error in devtools?",1
152,"TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

optimizer.iterations are not properly save & restored.",2
153,"Design choices:
- Choose how to compute the inverse needed in the gradient formula in general, as one of the most compelling use cases of the exponential is when its argument is skew-symmetric, and in this case computing the inverse is just computing the transpose.",0
154,"`go test` recognizes three types of test functions: `TestXxx`, `BenchmarkXxx`, and `ExampleXxx`.",0
155,"Yes, searching this encoding is no longer supported.",0
156,> I think the underlying problem here is that std does not have the concept of constants on memrefs.,2
157,What information can I add to help solve this issue?,1
158,"this idea matches pretty much our thoughts about adding ""structure"" to Pytorch to provide stronger types and memory layout and bridge data and model better with transformation libraries.",0
159,Also in this case we could even code a port assignment service on our own and only reserve free ports.,0
160,Focussing the window makes the text field disappear and the rename/file creation operation is aborted.,0
161,Also tricky is how you would present a failure to the user in this case.,2
162,"Replacing libtorch_latest with libtorch_1.0.0 solves the issue in my machine, though the reason is not clear.",1
163,-eph are there some recommendations on what patterns should be a part of Dialect 1 -> Dialect 2 conversion and what patterns have to be appended separately?,0
164,"Continued from 

> [-hykin] I spent years on a library (which I finally published just last week) to make it way less painful.",0
165,I had some incompatibility warnings too.,0
166,"I guess adding the regular select ops flag does the similar job as `RandomUniform` and `Mul `are already a part of [Select TF Ops]( Please check this [gist](

For reducing the binary size of the TFLite model consider 

Thanks.",2
167,"[CleanShot 2022-11-30 at 13 44 37 (1)](
vs WebStorm:
<img width=""680"" alt=""CleanShot 2022-11-30 at 13 45 20 (1)"" src=""",0
168,Let's use this issue as the umbrella for now.,0
169,">  I think that nvidia-smi does not list GPU processes when used within Docker (as in my case)

I see, thanks!",2
170,Thanks for the additional insight!,0
171,"In order to implement a feature like this, we may need to:
* do modification on the lookup table ops to make it trainable.",0
172,"Switch to Chinese typewriting mode and type `cmd + ``
5.",0
173,"> 
> Please share simple stand alone code for us to replicate the issue faced.",0
174,"Hi  

Thanks for the information.",0
175,"[Link](
Thank you.",0
176,I placed your code snippet before training a tensorflow tf2.0) model in jupyter notebook moving from another jupyter notebook.,0
177,It looks like the Windows CI build uploaded to the incorrect location.,2
178,Are you working with a system that requires 8-bit operations Only?,1
179,"On my specific system, as it has a s***ty GPU, it runs in a much earlier version of Python environment as root as you can see.",0
180,"So basically we want to look at the stochastics of 

!",0
181,I believe this works similarly to the solution I'd posted above.,1
182,"In the case of `f` being `sparse.sampled_addmm`, `f` maps unspecified values to unspecified values (read: implicit zero values), explicit zero values to explicit zero values (by the definition of `spy`), and non-zero values `v` with `index` to `alpha*(mat1)[index] + beta*v`.",0
183,Then you should try creating a separate .vscode/settings.json file for each folder(project).,0
184,Interesting -  and  are right.,0
185,I just need a way to disable triple-click.,1
186,I mentioned the architecture of the model.,0
187,if that doesn't work can you please give me your exact steps (as if you are writing a shell script or Dockerfile) to reproduce the issue you are seeing.,2
188,Adding [linux].,0
189,Downgraded from 3.10.9 to Python 3.9.6.,0
190,But to me it's all infuriating.,0
191,It's quite possible that this might be a cleaner way to do it.,1
192,"Everything works fine with the the CPU version of ""minimal example"", but the GPU version of ""minimal example"" of libtorch gives a segmentation fault while deconstructing.",2
193,"`densify` is internal to `gradcheck`, so there is nothing to undensify outside of the gradcheck.",0
194,"In other words, what we need may be a feature like this:


**Will this change the current api?",2
195,This is exactly what `densify` enables.,0
196,Hopefully it won't require any changes to the source.,0
197,"If so, I can take it up.",0
198,Large logs and files should be attached.,0
199,"The code I posted does not use the built-in metric monitoring, and does not use anything from tf.metrics, so it seems that there is nothing to reset.",2
200,"I see the ""author-verification-requested"" label, but not sure what that means.",1
201,Should be #128355,0
202,"Same problem in CUDA 12.2 + CUDNN 8.9.7 + torch 2.2.0

reinstall torch == 2.1.x solve the problem",2
203,"And the launch config properties related to terminals are part of the debugger's JSON schema, not part of VS Code's schema.",0
204,"Sincerely,
Vladimir Zaigrajew
pon., 26 gru 2022 o 09:34 SuryanarayanaY ***@***.",0
205,I have also installed the Cuda Toolkit as well.,0
206,Default device is the device you are setting with `torch.cuda.set_device()`.,0
207,Same for TensorFlow Java,0
208,[2023-01-11_08-53-03 (1)](,0
209,I encountered an issue where I need to meet the 80% test coverage criteria.,0
210,That can come later.),0
211,"In the analytic path, reading from how backward is implemented for `self` we would have that
`J[f_11, x_11](x) = 1`.",0
212,"Turkey, no vpn, no dns setting on the modem or at ubuntu.",0
213,We have tried a roughly implementation at  based on class `MutableHashTableOfTensors`.,2
214,"Even the argument of ""TF is better suited for production"" doesn't hold anymore, in fact we are shooting ourselves in the foot with bugs like this one which even after many years, are still not fixed.",0
215,I think it is related to `py_function`.,2
216,"I'm getting a bunch of these:



Will revert to an older version...",0
217,that's the reason why I asked for normalization features.,2
218,Then we have model parallelism where different layers will be scattered across multiple devices.,0
219,Could you please refer [this]( comment and let us know if that helps you.,2
220,I have an even stranger issue.,1
221,[snip_20161103210238](,0
222,The terminal shows up.,0
223,"`target = [0, 1]`, `weight = [w, 1]`), weights have no effect
- When respective target is onehot (e.g.",0
224,"I definitely know about Hyper ;) yet Code's implementation is currently much more solid though, and given each one's track record in stability and performance I'd rather bet on Code.",0
225,I have the exact same issues.,1
226,Also letting everyone know that download takes absurdly long in the Netherlands.,2
227,My use case is described in #105045.,0
228,2023-05-05 02:37:58.271133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.,0
229,I created  and was directed to here.,0
230,"---> 99   functions = saveable_view.list_functions(saveable_view.root)
    100   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)
    101   if signature is not None:

~/tf2/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj)
    152     obj_functions = self._functions.get(obj, None)
    153     if obj_functions is None:
--> 154       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access
    155           self._serialization_cache)
    156       self._functions[obj] = obj_functions

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self, serialization_cache)
   2711     self.test_function = None
   2712     self.predict_function = None
-> 2713     functions = super(
   2714         Model, self)._list_functions_for_serialization(serialization_cache)
   2715     self.train_function = train_function

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)
   3014 
   3015   def _list_functions_for_serialization(self, serialization_cache):
-> 3016     return (self._trackable_saved_model_saver
   3017             .list_functions_for_serialization(serialization_cache))
   3018 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)
     90       return {}
     91 
---> 92     fns = self.functions_to_serialize(serialization_cache)
     93 
     94     # The parent AutoTrackable class saves all user-defined tf.functions, and

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)
     71 
     72   def functions_to_serialize(self, serialization_cache):
---> 73     return (self._get_serialized_attributes(
     74         serialization_cache).functions_to_serialize)
     75 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)
     87       return serialized_attr
     88 
---> 89     object_dict, function_dict = self._get_serialized_attributes_internal(
     90         serialization_cache)
     91 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)
     51     # the ones serialized by Layer.",0
231,Does it reproduce on github.dev too?,1
232,"**[Contributing](

- Do you want to contribute a PR?",2
233,Also Our Tested configuration for latest version i.e TF2.12 are CUDA 11.8 and cuDNN 8.2.,0
234,"[Screenshot from 2021-06-28 17-07-23](

I cannot find a setting that changes the highlight color, any suggestions?",1
235,"Unfortunately, I can't determine which test cases cover which parts of the code simply by looking at the coverage UI.",1
236,"<h2>How to patch (on a Mac)</h2>

_Known to work on VSCode Version: 1.79.0-insider (Universal).",0
237,"Editor note: This is #17139

----

>  did you try after deleting `CMakeCache.txt` and `CMakeFiles` ?",0
238,This obviously not what you would want for TF.,0
239,Are there any updates?,1
240,We see that with the PyTorch implementation the effective learning rate from .75 to 2 times the learning rate the user provided.,0
241,At least extension can be useful until VSCode core provides stronger syntax parser.,0
242,"We typically only tolerate this when there is very strong need for it and try to avoid this at all costs 

I think it is safe to make this async.",0
243,I would like to get results like this from a shuffled dataset.,0
244,"-->

<!-- Describe the feature you'd like.",0
245,"the issue was posted to Keras git (link shared - 12 days before), but till today, no solution.",1
246,Well if I scroll away from the position and step the yellow current line gets revealed.,0
247,"I went through the SciPy implementation for `matrixexp` (named `expm` in SciPy), and have a few questions:
- the implementation is added for sparse arrays as well.",2
248,"> The PP in PyTorch basically pulls in the PP work from fairscale, although if you'd like to test things out in PyTorch you can always use the nightly builds to use the current PP implementation in master.",0
249,"* It might be that this package is called `tensorflow-base` or something else to your liking
2.",1
250,I mainly use IntelliJ IDEA and Notepad++.,0
251,"> You are receiving this because you were mentioned.Message ID:
> ***@***.",0
252,"Personally I would love it if double click stays the same, triple click selects everything up until the nearest space OR line-break OR bracket/parentheses (so that you can easily select a whole URL for example by triple clicking, while still being able to select just one part of a URL by double clicking) & then four clicks for the whole line, five for the whole paragraph, etc.",0
253,Tested my internetspeed and it was 750Mbit/s download speed.,0
254,"Unzip the Android project ZIP file in the link: 
2.",0
255,This would be much more straight forward if the test item can explicitly link to `DocumentSymbol`s.,0
256,"The implemenatation of raw_ops.floor_div as mentioned in the document is right way and it internally calls` gen_math_ops `code file, which does not use the logic of  
However, I have created a PR to changed the implementation of mlir/tosa floor_div.",0
257,You can use command 'select all occurrences of Find Match' to select all occurrences in a single file.,0
258,How do you exit the TF processes?,1
259,"I tried all combinations of `tf.keras.backend.clear_session()`, `tf.compat.v1.reset_default_graph()`, `gc.collect()`, `close()` the session, `tf.compat.v1.disable_eager_execution()`, and other solutions that I found online, but none of these really solved the issue.",1
260,"Related issue: #34105

Topic on StackOverflow: 


Possibly relevant theme color overrides: [Settings for Theme Colors](

My experiments in **settings.json** (not very successful):",0
261,"> given we already put a `vscode.lock` file into the workspace storage folder whenever an instance is running, I wonder if the js-debug extensions couldn't simply scan the <user data>/User/workspaceStorage folder for a folder that contains such a file.",1
262,This heuristic works most of the time but we have seen situations where the column indicator shows up even when it shouldn't (e.g.,2
263,"I'm planning to write HowTo in the next couple of days, but you can figure it out from source code, that is very simple and straightforward.",0
264,I would like to re-open this.,0
265,I assume that's how the [Syntax Highlighter extension]( works since EvgeniyPeshkov/syntax-highlighter.,0
266,I would need to look up the proper way to do it first.,1
267,"> I had checked the guide to reduce binary size before, the thing is my laptop is freezing in the middle of the build with bazel when I set SELECT_TF_OPS

Regarding freezing issues during Bazel builds with the SELECT_TF_OPS flag, it's advisable to consider using a more powerful machine with robust CPU and memory configurations for smooth compilation of model,  incase they are complex.",2
268,Thanks for your support.,0
269,The 'Help > Report Issue' dialog can assist with this.,0
270,"Here's the training function using my own loop (B):
~~~
def fit_model_b(model, dataset, normalize_fn):
    trainset = dataset.sample(frac=0.8,random_state=0)
    devset = dataset.drop(trainset.index)

    optimizer = tf.keras.optimizers.RMSprop(0.001)

    bsize = 32
    for ep in range(100):
        trainset = trainset.sample(frac=1)

        for i in range(0,len(trainset.index),bsize):
            x_batch = trainset.iloc[i:i+bsize]
            y_batch = x_batch.pop('MPG')
            
            with tf.GradientTape() as tape:
                y_batch_pred = model(normalize_fn(x_batch), training=True)
                loss = tf.keras.losses.MeanSquaredError()(y_batch.values, y_batch_pred)

            gradients = tape.gradient(loss, model.trainable_weights)
            optimizer.apply_gradients(zip(gradients, model.trainable_weights))
        
        print('{}\tloss={:.2f}\tfit-mse={:.2f}\tdev-mse={:.2f}'.format(ep, loss.numpy(), 
                    eval_model(model, trainset, normalize_fn), eval_model(model, devset, normalize_fn)))
~~~

I've included a collab with the full source code at the bottom.",0
271,"So std dialect ops are going to be in the legal target set for these conversions - hlo to lhlo, and already lhlo to linalg.",0
272,"We have a short term goal about adding quantized support to PyTorch (requiring a new kind of dtype), and making it *possible* for people like you to play around with alternative backends, but in the medium term, building a generally usable, publicly available API for doing backend work is not a priority.",0
273,Thanks  for your solution.,0
274,Could you also use colab fallback as a workaround?,2
275,Yes Hyper is planning on moving to xterm.js for v2.,0
276,"Hi , 
I have been trying to create a new class  `_SymmetricPadNd()`  in the `padding.py` file.",0
277,">
> Thank you!",0
278,Transforming std.const to lhlo.const is just hiding this omission by escaping to LHLO.,0
279,Having the source file and the test file open at the same time seems like it would be a pretty common scenario so I'd like to support it.,2
280,"-israyelyan 
This issue seems to be Keras issue.",1
281,probably can share a workaround if any.,2
282,"Suppose the developer made a very robust function `f` that projects all the inputs/gradients onto the right domains, this means that the function does support masked semantics but can tolerate inputs/grads not in the domain in a differentiable manner.",0
283,"So if we have a logical batch size of 64 and a physical batch size of 8, ""apply_optimizer_in_backward"" will not distinguish between the microbatches.",0
284,"However, that's not a very macOS like experience as here is what happens in a native app.
!",0
285,"Hi, this is not what I am looking for.",1
286,"What ""other ways"" did you mean in `potentially be presented with such an input in other ways, and so making -hlo-to-mlir-hlo generate hlo.const isn't a real solution.",2
287,> Do we agree that gradcheck producing True with masked=False is incorrect?,0
288,(Right could be remote from text.),2
289,I finally got my Stable Diffusion working by simply following this tutorial:,0
290,"## Environment

PyTorch version: 1.9.0+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.12.0
Libc version: glibc-2.26

Python version: 3.7.10 (default, May  3 2021, 02:48:31)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
Is CUDA available: False
CUDA runtime version: 11.0.221
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.9.0+cu102
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.10.0
[pip3] torchvision==0.10.0+cu102
[conda] Could not collect
cc",2
291,"However, this is a problem with all Pytorch calculations, not just this function.",0
292,GPU1: Nvidia GT1030 2GB.,0
293,"We have the following scenarios:
- Simplest.",0
294,"You can just make an Optimizer locally that does whatever you want it to do, and define a local ""apply_mask_in_backward"" that instantiates that Optimizer and appends it to param._in_backward_optimizers, and pass model.parameters() to this function.",0
295,"That would be an interesting feature to add to tensordict, but would need some discussion.",0
296,This means that the contribution of the same data point to the overall loss is dependent on the other members in the batch which is not the right behavior.,0
297,I have confirmed that the same error occurs on at least two different Android devices (Pixel 7 Pro and Galaxy S20 Exynos).,2
298,In the meantime I recommend hooking up a keybinding to toggle maximize on the panel if you haven't yet  I use it all the time 😃,0
299,The problem is not there with 2.1.2.,0
300,Can anyone gives a hint?,1
301,"** I create my .tflite file from this Google Colab Notebook [here](

**2.",0
302,"If it did, its valid lowering for codegen would have to again pass through loops, which are themselves not in the std dialect.",2
303,"Compute (-torch.nn.LogSoftmax(1)(input_t)[range(3),target_t]*weight[target_t]).sum()/weight[target_t].sum()



## Expected behavior

I would expect the denominator in the expression to be the number of rows in the minibatch.",2
304,"For example, with `randn` (complex dtype support is missing for other ops as well (e.g., `rand`, `sqrt`, etc))

raises the following error from codegen



I wonder if there's any context to why complex dtypes are currently generally not supported in Inductor.",2
305,Also having the external terminal and Code's integrated one behave in exactly the same way is a non-negligible feature.,0
306,>Are you referring to my bug reproduction script or the topic of this rfc?,1
307,"You can get the compiler by installing the trail version of Intel Parallel Studio XE, and follow this guide 
2.",0
308,Throwing in [Troubleshoot Mode]( as option to test and different tracking protection settings.,0
309,sadge,0
310,Auto-run a test whenever the user changes the referenced location.,0
311,Would be an amazing productivity-customization feature.,0
312,`densify` is used only in the numerical path of gradcheck where differentiability is irrelevant.,0
313,"Most files are being opened here:



And then we use normal `fs.read` primitives from node to read and `fs.close`.",0
314,The IR is valid and the mix makes sense and supporting such a mix is pretty important.,0
315,The problem is more important now with the introduction of new distribution strategies like MultiWorkerMirroredStrategy.,0
316,"Since this involves a paper, maybe can this go into `contrib`?",2
317,">
> —
> You are receiving this because you were mentioned.",0
318,"Sure the editor ca do lots of fancy edge case stuff, but the search functionality is poor compare to other IDEs.",0
319,Agreed.,0
320,Do a worskspace search for anything.,0
321,"CUDA is different because there is dedicated matmul ptx instructions but only for float not complex, and the required layout is for real imag to not be interleaved.",0
322,"Hi , 

Thanks for looking into this.",0
323,"As yarn cannot reserve ports, some other rogue processes can take the port before the tensorflow server.",0
324,"Hi,

Thanks for all the details!",0
325,"Lastly, if you insist to have this pattern, it should at least live in a different file.",0
326,"Thus, the search viewlet could show only hits in the current file.",0
327,"But the output is still the ""RawPipeline"" Problem.",2
328,"> Hi  
> 
> There is a known issue of tflite model maker installation if you are using python >=3.10.",2
329,">
> —
> Reply to this email directly, view it on GitHub
> <
> or unsubscribe
> <
> .",0
330,"There's also [this paper]( which seems to have been published during the same time, but did not make it to publication.",0
331,If you are using conda you can do so like this:,0
332,"regard to a numerical stability issue, what is your suggestion to solve this issue?",2
333,"Just FYI, I exported all paths to their appropriate paths while I was at it.",0
334,still relevant,0
335,"However, when asked if it is still relevant, i will kindly say that it still is pleading for the issue to remain open.",0
336,"Subsequent clicks should expand the selection logically, not just to the entire line as it currently does.",0
337,"If it is not, please file a new bug for your issue.",0
338,"<img width=""1137"" alt=""image"" src=""

From what I see now, there is a background on the line for the current result, so this *should* resolve this.",0
339,"### 🚀 The feature, motivation and pitch
I see Inductor cpp codgegen `DTYPE_TO_CPP`, `DTYPE_TO_ATEN` do not support complex dtypes.",0
340,"Update: I found this semi-decent hack that seems to force VSC/TSC to expand the types in some cases:

`type Id<T> = {} & { [P in keyof T]: T[P] };`

Wrapping a complex type with this mapped `Id` seems to work.",0
341,Could you please move this to closed status as it is tracking  in keras-team/keras repo.,0
342,"Don't use [language] sub-group, just put a single parameter _""files.encoding"": ""CP437""_.",0
343,: search with accented characters is partially functional: it does not return all the files containing search string with accented characters.,0
344,I opened (the now closed) issue #130978 due to this bug.,2
345,"Yes: this is still driving me crazy, and having an option to simply disable triple-click would be more than sufficient.",1
346,I would like to ask for one more feature to normalize the values.,0
347,Would it be fast enough to call on every step?,1
348,"**
  - Comparing `Function` or `Type.Method` against already loaded tests is easy: all I have to do is find a test item in the same directory named `TestFunction` or `TestType_Method`, respectively.",0
349,"Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
[1,0]<stderr>:type_id: TFT_OPTIONAL
[1,0]<stderr>:args {
[1,0]<stderr>:  type_id: TFT_PRODUCT
[1,0]<stderr>:  args {
[1,0]<stderr>:    type_id: TFT_TENSOR
[1,0]<stderr>:    args {
[1,0]<stderr>:      type_id: TFT_INT32
[1,0]<stderr>:    }
[1,0]<stderr>:  }
[1,0]<stderr>:}
[1,0]<stderr>: is neither a subtype nor a supertype of the combined inputs preceding it:
[1,0]<stderr>:type_id: TFT_OPTIONAL
[1,0]<stderr>:args {
[1,0]<stderr>:  type_id: TFT_PRODUCT
[1,0]<stderr>:  args {
[1,0]<stderr>:    type_id: TFT_TENSOR
[1,0]<stderr>:    args {
[1,0]<stderr>:      type_id: TFT_HALF
[1,0]<stderr>:    }
[1,0]<stderr>:  }
[1,0]<stderr>:}
[1,0]<stderr>:
[1,0]<stderr>:	while inferring type of node 'cond_39/output/_24'
[1,1]<stderr>:2022-10-21 06:40:26.918500: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed.",0
350,Moving on.,0
351,That is for me what the whole group concept is about.,0
352,So the two `provide` handlers seem like the best choice when I have to scan.,2
353,Have a look at [this]( reference for more information on quantization.,0
354,"The main process PID is written to the lockfile, but I don't do a liveness check on that yet.",2
355,"I have no comments on that either :) 

Just to provide context in IREE.",0
356,"OK, the backward formula for `self` does not account for the mm part, right?",2
357,The same problem.,2
358,Are you on this issue because of an 'undefined reference' error?,1
359,"This naïve approach results in the problem, that a column indicator is shown even if there is only a single statement in a line (and there is no real value in showing the column indicator).",0
360,"I added a --define=build_with_mkl=true flag but that wasn't it, but it seems it got further

produces eventually:


, can you please take a look?",0
361,My version of CuDNN is version 12.,0
362,"Add matrix power as implemented by [numpy.linalg.matrix_power]( matrix exponential as implemented by [scipy.linalg.expm]( and matrix logarithm as implemented by [scipy.linalg.logm](


- [x] matrix_power
- [x] matrix_exp
- [ ] matrix_log (For an implementation see 
- [ ] matrix_sqrt (For an implementation see 

cc",0
363,Have you fix it?,1
364,Ok!,0
365,But looks like the draft PR raised by  is for the 'push' solution?,2
366,"see [this]( and [this](

So if pytorch already has a global arbiter `torch.cuda.set_device`, albeit deprecated, why not just use it to set the current device and have all the ops move the data to that device before doing any operation (most of the time they won't need to do).",1
367,SSIM is also very useful on other image tasks like predicting depth.,0
368,Hey i am having the issue   mentioned above.,2
369,"iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.6.3
- CUDA/cuDNN version: 10.1
- GPU model and memory: nvidia gtx 1080 

**Describe the current behavior**
I'm trying to learn the new tensorflow 2 API using the following tutorials:
[1] 
[2] 

Currently, the regression model based on [1] works really well when I use the model.fit() API.",0
370,"You cannot do anything to allow it to work completely (aside from converting all your files to standart encoding, which is not possible in many cases).",0
371,"I've just updated my VsCode to ""April 2019 (version 1.34)"", and the issue is still there...
Can we hope a solution for that?

!",2
372,Similar issues seems to be present when converting split operators as well.,2
373,"Notice the line highlight showing up briefly then disappearing.

!",0
374,as an update to this: we have bfloat16 support natively in PyTorch now!,0
375,The same does not happen when running these models on CPU.,0
376,It would provide better utilization of GPUs and you don't have to worry about `.to()` either.,0
377,But depending on how your system is setup and how you installed PyTorch this may or may not work for you.,1
378,"In addition to the cases here, one thing that IntelliJ gets really right is case sensitive matching of partial words and priority for local symbols.",0
379,"While mask depends on s, it is non-differentiable, and autograd will ignore it because mask.dtype is non-float/non-complex.",0
380,"When I tried to use my phone to download the package from  using my mobile data, the speed was pretty good... What's wrong with that?",1
381,cc:,0
382,But maybe there is a setting which does what you describe that pulls implementation ranges for tests of active editors.,1
383,"I am downloading in Japan using the recommended package manager (conda), trying to get the 1.4.0 update because of the PIL import error for newer PIL, but the download speed is absurdly slow, it times out because it's so slow.",0
384,">  If not, it's weirder to have to run a separate pass do std to hlo, and I don't see how this solves the original issue

Why doesn't it solve the original issue?",1
385,I always thought that PyTorch follows the principle of correctness first.,0
386,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.10

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Nvidia Geforce 1080 Ti

### Current Behaviour?",0
387,CC -roy,0
388,"Recall, in non-masked semantics, the ""sparsity pattern"" of a tensor (in terms of its indices set) does not define a mask because non-masked semantics is layout-agnostic.",0
389,What's going on with the unusable download speeds from Germany?,1
390,"Currently, it always select the line break (if there is one).",0
391,I mean really!!!,0
392,Could you please provide how CUDA/CUDNN was installed on your machine ?,2
393,"Both global seed and local seed together
> decides the Random Number generation.The local seed changes the value with
> every epoch but it will be in conjunction with global seed.Please refer to
> shuffle API here
> <
> I just attached a gist here
> <
> to test how global seed and random seed works.I run the same code in 3
> different cells and all producing same numbers in order.Then I tried with
> restart runtime and also disconnect and delete runtime and every time the
> results are in same with order also.This is a simple demo to explain how
> random seeding works in tensorflow.Please confirm whether it is useful for
> you.",0
394,"The only drawback is that one loses the original sparsity pattern of inputs that the input function may use (as is the case for `sparse.sampled_addmm`) but in the context of non-masked semantics, where the layout is considered only a storage optimization, losing the original sparsity of inputs is insignificant.",0
395,Let me know if you have ideas on how to improve this and I can reopen,2
396,"feature_request_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct:  -->
<!-- Please search existing issues to avoid creating duplicates.",0
397,Then the conversion to memref does work.,0
398,"There are two threads at the moment:
* 
*",0
399,A completionist would add custom setting for each specific font/weight/color/background for aspect of line and selection of match.,0
400,I see your points and I'm OK with letting VSCode/the user decide when to pull that info.,0
401,Can you please also allow launch configurations to specify terminal group when using the integrated terminal for debug and start/stop?,0
402,"Atm, I tend to think it is the latter as `masked=False` means that the sparsity pattern of the inputs should be unessential while in the case of `torch.sparse.sampled_addmm` it will be essential if we are going to change its backward formula, or if we would need to apply perturbations under the mask defined by the input indices (that begs using `masked=True`).",1
403,I also would find this extremely helpful as i cant even figure out the correct naming scheme to even select the font i'd want without having a list there to choose from...,2
404,"Upvote for this, matrix exponential `expm`, logarithm `logm` and (fractional) matrix power `powm` which includes square root (with negative powers support)",0
405,Go has an established naming convention for associating examples with what they are an example of.,0
406,<br>Feel free to remove the `Stale` label if you feel this was a mistake.,0
407,"So, the requirements for `func` are more subtle.",0
408,"My understanding is that applying optimizer in backward is algorithmically incompatible with gradient clipping, where there is a dependency on the total norm computed over all gradients before running the optimizer step.",2
409,"To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.",0
410,"If not, it's weirder to have to run a separate pass do std to hlo, and I don't see how this solves the original issue.",1
411,"The button is using same command ""editor.action.changeAll"" as used by ""change all occurrences"".",0
412,"When you do ""Find"" within a file, the editor places a faint background highlight on the entire line (good).",0
413,Closing in favour of,0
414,"As I already mentioned a couple of times above, I don't think we should choose to not deal with std.constant when it's presented to this pass.",0
415,"Just FYI, Anaconda is just like Python as said previously.",0
416,**,0
417,Thanks for hearing me out.,0
418,"Please do the following steps before replying ""me too"":

1.",0
419,This is not related to run to cursor command but with our heuristic of when to display the inline instruction pointer.,0
420,"I am the author of the test explorer implementation for `vscode-go`, so my interest is how this relates to Go.",0
421,I am not sure that such a fix will cut it.,1
422,"Apply temporary highlight to the matched line, when selected from SEARCH.",0
423,"With a reproducible script and a full backtrace with the error, we can help you better, but you can try
1) Run latest PyTorch and onxruntime-directml ml and post the exact compatibility error you are mentioning.",0
424,Native CUDA-accelerated basic CSV-parsing could also be nice (especially if combined with mmap-based file reading?).,2
425,"Here is a work-in-progress  Bart port - it works just fine, just needs a bit of cleanup...",0
426,"Allow the user to see what symbols are linked to a test, similar to (or incorporated into) find references.",0
427,"`gradcheck` produces `True`, which means that both numeric and analytic Jacobians are incorrect, since analytic formula for grads wrt `self` is incorrect because it misses the projection step and/or, if not for the sparse semantics, it should include the subgradient generated by the part `alpha * (s != 0) * (mat1 @ mat2)`.",0
428,The question might be just about file organization.,0
429,Over an hour and I'm about 60% through it.,0
430,I would suggest using the implementation from  that will be more reliable.,0
431,strictly speaking this pattern is STD-> LHLO.,2
432,"If you have time to do this in April, I could enable it for chrome-debug so we can try it out.",0
433,"Hmm so in that case I guess the solution would be to check if the window has focus, and if not, just create a file/folder with a preset name",1
434,So we can try to reproduce this issue,0
435,can you post collect_env from your working PyTorch-2.1 installation?,2
436,I think   is not opposed to adding such a pattern.,2
437,😀,0
438,I think that PyTorch is moving toward `register_post_accumulate_grad_hook` as the way to implement optimizer in backward.,1
439,Are these worth porting into ATen?,1
440,"If std.constant can be presented as input to this pass, you have no choice but to deal with it here.",0
441,"This bug has been fixed in to the latest release of [VS Code Insiders](
, you can help us out by confirming things are working as expected in the latest Insiders release.",0
442,"Could not reproduce this issue:

Installed cuda+cudnn via:




Installed torch 2.2 torchvision 0.17.0 via pip.",2
443,"I am not aware of a specific way to figure this, the problem here is that we also want to identify the mounted paths.",1
444,"-->
Does this issue occur when all extensions are disabled?",1
445,"So, again: please create a new issue.",0
446,"Hi  , this is a very old issue that everyone is facing in TF 1.x as well as TF 2.x, it seems to be a design flaw and the TF team doesn't seem to care about fixing (I have been facing this issue for more than 2 years now).",2
447,Nothing changed here right?,2
448,I'm facing the same issue downloading from Malaysia.,2
449,Please find the reproducible [gist]( in TF 2.13.,0
450,"> Making `tf-mlir-translate -hlo-to-mlir-hlo` pipeline generate xla_hlo.constants would also make more sense, I think.",2
451,Why shouldn't one have such trivial intermediate patterns for canonicalization or to facilitate / trigger other conversions towards your legal target?,1
452,"If you look at the reported tensor `b2` it has the wrong shape of `(2, 6)`, but should have been `(2, 2)`.",0
453,"Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_BOOL
    }
  }
}
 is neither a subtype nor a supertype of the combined inputs preceding it:
type_id: TFT_OPTIONAL
args {
  type_id: TFT_PRODUCT
  args {
    type_id: TFT_TENSOR
    args {
      type_id: TFT_LEGACY_VARIANT
    }
  }
}

        while inferring type of node 'while_3/body/_5262/while_3/while/body/_16348/while_3/while/cond_1/output/_22035'",0
454,Please create an issue with title prefixed by `[PT_BREAK]` in [`pytorch/xla`]( and link to to this PR.,0
455,"Is there a way to unregister a new scalar type after registering it [like so here](

For my specific needs, it would be very helpful if we could provide a scalar data type factory api, using which the basic setup to add a new tensor type could be done while still keeping all the heavy-lifting in the c++ extension.",2
456,"Suppose I have a function `f` that accepts sparse differentiable inputs and we have backward implemented for it, WHICH IS CORRECT in the domain it is supposed to be used in.",0
457,Really missing feature!,0
458,Think of this like cross-hairs.,0
459,"</details>Click<a href="" here </a> to manually regenerate this comment.",0
460,"Thanks for letting us know, I think my fix is not doing anything useful then.",2
461,"There are two threads at the moment:

> huggingface/transformers#8771 (comment)
> huggingface/transformers#9323 (comment)

Based on this comment, it does seem like pipeline parallelism might be a good fit here since it has been used to train transformer models.",2
462,some aspect is also padding / alignment by memory page size (which is OS/machine-dependent) for mmap sometimes,0
463,"> If you can't make it work, just disable all the context menu options that don't work without focus and call it a `hotfix` until you find a way to actually make it work.",0
464,"Step should ""recenter"" the current ""yellow"" instruction if it is out of
view.",0
465,This comment will trigger a new check of this PR.,0
466,Inclined to think the code to lacks any notion of a visual hierarchy in presenting search results.,0
467,Can you explain more about why this isn't sufficient?,1
468,"This can be done by right click on the selection and choosing ""Change all occurrences"" from the context menu (or CMD+F2).",0
469,Or maybe a Dense Tensor + Int Tensor (for storing string lengths of array elements) - btw don't know if Nested or if there's some other built-in representation for representing Padded Tensors,1
470,"Please take this up, .",0
471,"I have pushed a somewhat weak workaround to ignore any path contained in `/Volumnes` when using `fs.watch`:



But I feel this is just a bandaid, because:
* imho a native crash should never ever occur when using an official API no matter what, at the very minimum the upstream component should log an error and refuse to watch but never just crash
* looking at all the other issues that were marked as duplicate of this one, I see users having mapped network drives on other paths than `/Volumnes` possibly using symlinks, so I doubt we would catch all cases here

Back to you Deepak, I think this needs a different upstream fix if possible and better understanding of the origin of the assertion.",2
472,The write-up is obviously based on my current understanding of PyTorch internals.,2
473,"From the DISABLED prefix in this issue title, it looks like you are attempting to disable a test in PyTorch CI.",0
474,what is the situation in the new (inspector) protocol?,1
475,"A specified ""presentation property"" in the ""runInTerminal"" request makes only sense if the specification is ""universal"" (independent from VS Code).",0
476,"> Hi -arm
> 
> I did observe that it does not add Quantize/Dequantize stubs for intermittent runs.",0
477,"If you have questions, please reach out to  /  / .",0
478,"Thank you for the follow-up, .",0
479,I'm completely fine with this as long as this pattern can run as part of -hlo-legalize-to-lhlo.,0
480,Still working at it.,0
481,"This would also enable matrix square root, though for matrix square root there seem to be specialized approx algorithms based on newton iteration.",2
482,"In the meantime, you can use the preview in  to get the links to the latest archives.",0
483,I don't know if this is helpful for you though.,1
484,Is it possible to share the error stack trace and TFLite model to reproduce the issue?,2
485,"However, I
don't think this is the best way to do it.",1
486,"So, I understand what I am doing is not the intended use for the experimental_select_user_tf_ops flag.",2
487,Comparing results with skimage might be helpful.,0
488,Has the libtensorflow build for Windows been deprecated?,1
489,"We don't have tutorials yet, but will have some as part of pt1.8.",0
490,"`tf.config.set_visible_devices(physical_devices[1:], 'GPU')`

Please try this and let us know if ot works.",0
491,The reason I am not over-fond (as mentioned above) of the centering alone (as cue) is this works poorly on large visually busy screens.,0
492,"And, in fact, it breaks on functions like `sampled_addmm`.",0
493,We unfortunately cannot use tf.function in our scenario because we call into a python library.,0
494,I will let you if I make some progress.,0
495,It is necessary to use distributed tensorflow with resource managers that do not reserve ports (such as Yarn).,0
496,"what I was thinking was to be able to launch the terminal by itself using `code --term` or something similar, this would involve:

- Create the ""terminal window"" which has access to the services used internally by the regular VS Code window (this part would be particularly tricky)
- Modifying the CLI to support this
- Figure out how keybindings and other user settings interact with this special window

Before starting any of this it would probably be wise to get buy in from the team though (and it's not a priority yet so I wouldn't do this yet).",0
497,I think this could be implemented by having an action in the editor that invokes the global find in files but scoped only on the current file.,0
498,"See the limitation part of 

Did you try use `tf.funciton` instead of `py_function`?",0
499,Another torch/arrow related dataframe package with StringColumn support:  by,0
500,"I did `export LD_LIBRARY_PATH=""""` and now the script works.",0
501,"Build `keras` ....
4.",0
502,"The function `sampled_addmm` is parametrized by `self` and not by a pair `(self, mask=self != 0)`.",0
503,"I'm assuming the BartEncoderLayer/BartDecoderLayer are sequential and if so, wouldn't one GPU always be idle?",1
504,"""**",0
505,I think this is the key point for determining if the raised issue is valid or not.,1
506,"Secondly, I understand how shuffle works and how the
reshuffle_each_iteration parameter affects, however, I want to shuffle my
train dataset every epoch and also want to know how shuffle shuffles this
dataset every time, so using this parameter doesn't help me (looking at my
example
).",2
507,Just add below setting in .vscode/settings.json.,0
508,I would prefer `provideAssociatedTests(symbol: vscode.DocumentSymbol): ProviderResult<vscode.TestItem>`.,0
509,What part of the collect_env print makes you suspicious about the gpu?,1
510,This issue was closed because it has been inactive for 7 days since being marked as stale.,0
511,That's the point of this issue.,2
512,accidentally closed issue,0
513,This is applicable to Go.,0
514,"Put differently, when I click on a match in the SEARCH panel, my eye has no guide to find the matched text.",0
515,I'm having the same issue.,2
516,The only way I have found to get memory that has already been allocated back in that situation is to reboot.,0
517,"Right now to build Tensorflow from source, I used to do pip install the required packages mentioned under setup.py following the instructions mentioned [here](",0
518,Thanks for adding hotfix.,0
519,"> 
> `kill -9 125022`

 No, I think the main point of this issue is that we want a way to clear the tensorflow gpu memory usage while keeping the main python process.",0
520,maybe a rename or an ERROR event) when the remote filesystem is unmounting and this [line]( causes the crash since `kevent` fails.,1
521,In will try it out and give feedback.,1
522,"installing using an older version, which is not hosted on pythonhosted.org",0
523,To avoid this VS Code uses a heuristic: the column indicator is only shown if the same line is hit more than once.,0
524,do you mean ctrl+`?,1
525,Suppose `f.backward` assumes only sparse semantics in the backward.,0
526,"I just gave that a try (Ubuntu 20, Firefox 98) and guess what, it was not working unless I would reload the site with developer tools opened, but now it works.",2
527,"If it is, please add your environment configuration to this ticket.",0
528,"Many tasks, such as image reconstruction and image super-resolution, use SSIM and MS-SSIM as a loss function, and this helps enormously with the output quality.",0
529,This is fine and can already be implemented by the user with the provided API.,0
530,Country - Belarus.,0
531,Just checking if you have any updates on this issue.,0
532,I think it would be reasonable to use the same convention to attach tests and benchmarks to functions/etc.,2
533,I've prepared a reproducible code for the error.,2
534,if you can change pip install to direct package name on your server it'd be better I think,2
535,I used `bfloat16` as an example.,0
536,I'm changing the title of the issue accordingly.,0
537,"Anyway, I will check those articles you mentioned and implement SSIM/MS-SSIM in the following weeks.",0
538,Could you look at this issue.,2
539,"I think such changes are very likely to be rejected by the autograd team (the complexity of the needed changes is much higher than that of #97825 which was [rejected]( as too complex), so I find the discussion about removing the `densify` step without the support from the autograd team just too unrealistic.",2
540,"***>
>",0
541,"E.g., things that are likely to change in the new feature, e.g.",0
542,I was looking at another section of the docs and I wanted to see the git history for a particular page when I encountered this issue.,0
543,I`ve tried that already in our code but the we are still running into the issue.,1
544,"I already have a fallback solution to use in-memory DB if IndexedDB fails:



I am not sure if the blank page is really related to storage service failing, in the end we log an error but continue as normal with in-memory I would say.",0
545,"I've searched the same file I've been downloading and found at a mirror which lets 1MB/s download speed, what is going on with pytorch?",1
546,"Basically, I want to keep the history, but it must take long time to clone the whole repo.",0
547,"As per our
[GitHub Policy](
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub.",0
548,So centering is not enough.,0
549,"So how the iterator
is stored, because if it is stored as an object (pickle for example) then
it is not optimal for storage space (mentioned: Note however that the
iterator checkpoints can be large).",0
550,"Open this file in VSCode : 
`/Applications/Visual\
Studio\ Code\.app/Contents/Resources/app/extensions/node_modules/typescript/lib/tsserver.js`
2.",0
551,### 1.,0
552,"The SSIM is invariant to the value scale, but not everyone needs standardization, which would make the process slower than it already is.",0
553,I'm not sure we want this group concept proliferating to everything ?,1
554,"This should also clean up the grunginess around needing a BFloat16HooksInterface.h

As for timeline, our priorities regarding open backends are a bit subtle.",0
555,The difference to our current matching strategy is that true fuzzy matching allows to return a  result as matching if the search characters are part of the target word even if there are other characters in between.,0
556,The explanations makes sense from an implementation perspective.,0
557,"It already has been discussed in ticket  It is unclear why the option has been disabled in 

**Will this change the current api?",2
558,"> 
> Why?",1
559,Olive and ONNX optimizations still do not seem to work.,2
560,hash all of the strings in the array at once).,0
561,"1366     """"""
-> 1367     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
   1368     concrete._garbage_collector.release()  # pylint: disable=protected-access
   1369     return concrete

~/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   1282       # In this case we have not created variables on the first call.",0
562,I didn't run into that issue,0
563,** I clone [my files]( from terminal.,0
564,We're going to add some way of defining new operators which is not inheriting from Type and then filling in a bunch of methods.,0
565,Related:,0
566,That also enables non obvious scenarios like LS where we might need to know about tests that aren't opened on the machine you are running in.,0
567,did you try `bash ../tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn nccl caffe2 libshm gloo THD c10d` ?,2
568,"### 🐛 Describe the bug
The [PyTorch mnist example]( failed to work on a machine with preinstalled cuDNN, and manually install PyTorch 2.2.0 (`pip install torch torchvision`).",0
569,"Again, perhaps not new, but confirming what I see.",2
570,From what I understood all it does is giving the spec in some form to dist strategies which then start the server again.,2
571,"[ssim.txt](
Please note that I made no effort whatsoever for compatibility on any other system than my own.",0
572,"Similarly, if the tutorial is of use to the wider community, I would be happy to create a PR for that as well.",0
573,"I tried to use `git bisect` to narrow down the commit that introduced the error and arrived at commit [c1286fe88e14146a47d5a64d5de17f6479d2e21f](

> commit c1286fe88e14146a47d5a64d5de17f6479d2e21f
> Author: Sagun Bajra <sagunb.com>
> Date:   Fri Apr 15 09:12:51 2022 -0700
> 
>    Enable eager op as function mode in tf nightly.",0
574,"it was assigned to me so that I can review SSIM, and figure out whether we should add it into PyTorch core or not.",2
575,"A lot of `std` dialect op can be seen as the same level of abstraction as HLO, using the existing XLA compiler to target TPU from a tensor-level computation involving `std` ops could be useful to someone for example.",0
576,If you're pipeline is always including -std-to-hlo before -hlo-to-lhlo shouldn't this just work then?,1
577,"Tried with `1.3.1` and the latest `1.7.0`, both are slow with no difference.",2
578,The same problem on CentOS 7.,2
579,"Here's what I'm thinking:

- The extension creates a TestItem for `feature_test`
- The extension informs VSCode that this TestItem is associated with `feature_func`
- VSCode shows ▷ in the gutter next to `feature_func`
- When the user clicks ▷ next to `feature_func`, the TestItem `feature_test` is run

It is up to the extension to determine how to associate TestItems with DocumentSymbols (or ranges).",0
580,As previously [announced]( all future development of Keras is expected to happen in the keras-team/keras repository.,0
581,I'll see what I can do,0
582,Maybe a simpler check would be to see if the path starts with `/Volumes` for default mount point cases ?,2
583,[run-to-cursor](,0
584,"Note that if we want to start supporting complex dtypes we'd need to go all the way and start supporting them all throughout, not for just for `randn`.",0
585,"The current code works mostly just fine, albeit requiring a lot of code which I feel could be not required.",0
586,"To test this, create a VM, then run:

``python3 -c import tensorflow as tf physical_devices = tf.config.list_physical_devices('GPU')``

It'll probably work for you.",0
587,"Additionally, I am considering adding a configuration flag (disabled by default) that will automatically scan the entire directory for tests and source symbols when any file in the directory is opened.",0
588,Things become very complicated now because it's far from trivial to know which device the inputs need to be switched to.,2
589,The default action will disable the test for all platforms if no platforms list is specified.,0
590,"Even though you think it's the same issue, and you might be right, you're just causing noise and making communication between me and  harder.",0
591,"> 
> I got bored of trying after a week or so of trying to get it to work.",2
592,"~/tf2/lib/python3.8/site-packages/tensorflow_probability/python/layers/distribution_layer.py in __call__(self, inputs, *args, **kwargs)
    228   def __call__(self, inputs, *args, **kwargs):
    229     self._enter_dunder_call = True
--> 230     distribution, _ = super(DistributionLambda, self).__call__(
    231         inputs, *args, **kwargs)
    232     self._enter_dunder_call = False

~/tf2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in __iter__(self)
    518   def __iter__(self):
    519     if not context.executing_eagerly():
--> 520       self._disallow_iteration()
    521 
    522     shape = self._shape_tuple()

~/tf2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in _disallow_iteration(self)
    511       self._disallow_when_autograph_disabled(""iterating over `tf.Tensor`"")
    512     elif ag_ctx.control_status_ctx().status == ag_ctx.Status.ENABLED:
--> 513       self._disallow_when_autograph_enabled(""iterating over `tf.Tensor`"")
    514     else:
    515       # Default: V1-style Graph execution.",0
593,"Unfortunately, I am unable to apply the Process trick because I am running the model in Celery worker which is a daemonic process and it is not allowed to have children.",0
594,"I implemented the solution based on spawning a subprocess to run Tensorflow code and (as expected) it actually works, because all resources (particularly GPU memory) are released once the subprocess is destroyed.",0
595,Found other issues but the site always loaded.,0
596,`ext` correctly matches.,0
597,CC,0
598,"419     """"""
--> 420     return self._run_internal_graph(
    421         inputs, training=training, mask=mask)
    422 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    554 
    555         args, kwargs = node.map_arguments(tensor_dict)
--> 556         outputs = node.layer(*args, **kwargs)
    557 
    558         # Update tensor_dict.",0
599,Or perhaps there should be a way of switching some global flag that unless specified otherwise will switch all args to that device before doing any operation.,0
600,I would definitely support adding presentation or similar as an opaque option bag : ).,0
601,For the simplicity of the example let's say we have 2 gpus we want to split the model into.,0
602,4.,0
603,I am still not convinced we need to invent our own lock file though.,2
604,"Hence the ""needs design"" label.",0
605,"-->
There is a lack of any sort of visual hierarchy in walking through multi-file search matches.",0
606,Well typically if we do optimizer.step() we would want to log the gradient norm (I think that's basically a universal usecase).,0
607,"Otherwise, use `masked=True` to disable it.",0
608,I can make video tomorrow if needed.,0
609,I've try many things but the _experimental_select_user_tf_ops_ flag just doesn't seem to work.,1
610,"In a word-processing document, triple-clicking in a word selects the paragraph containing the word….",0
611,The problem for me it's that I don't have an encoding per language but per project.,0
612,"[

Can't find it in the previous location neither, last one is 2.15.0",0
613,"Also, I am still trying to remove small data transfers each time this is used.",0
614,stacks of 2D grids like we have stacks of 1D grids today) or something else you are working on.,0
615,I mainly use java and javascript:,0
616,SAD,0
617,"On 1.75.1 and not seeing any improvement, but presumably **On Deck** means upcoming.",0
618,Just when every function is called with which parameters?,1
619,How else would the user trust in sparse if the grads are potentially broken for a lot of functions in the `torch.sparse` namespace?,1
620,[Screenshot 2023-01-26 at 6 55 58 PM](,0
621,"The issue with the original repro is that it doesn't set the `reshuffle_each_iteration` argument to [shuffle]( By default `shuffle` will reshuffle, so you get a different order each iteration.",0
622,👍🏼,0
623,"----------------------

MP = Model Parallelism

1.",0
624,Like the original author I frequently have a very hard time finding where focus was moved to in the editor when stepping through search results.,0
625,"> 
> Thanks.",0
626,"This problem of serving both fast and expensive test discovery is quite reminiscent to tests themselves which resulted in the semi-lazy model we have today, so reusing this mechanism would make sense to me.",0
627,"We're writing tools to improve the developer UX, not language specs, so I think it's fine to go with convention.",0
628,- Monaco's own [Monarch]( looks like (without trying it) a conceptual hybrid between Tree-sitter and Sublime.,2
629,FYI,0
630,Should I take the same approach with `ATen` or is a general implementation sufficient?,1
631,This PR( already supports complex64 in Inductor cpp codgegen.,0
632,"[image](

It's opaque enough that without understanding all the utils that are used and computing it in the head, it's hard to understand what type is being represented.",1
633,"The most common me too's on this issue are:

* Build error: Undefined symbols for architecture x86_64: ""_MPI_Comm_rank"" on OS X (please go to #7065)
* Cannot build libtorch: SLEEF does not allow in-source builds #17181

----

Hello, I have been trying to install pytorch for roughly a week now, and I created a dual boot just for that purpose.",2
634,"I've already compiled OpenMPI (with CUDA) and I'm getting the following:



I get the same errors as -5 if Open MPI wasn't compiled with CUDA though.",0
635,"I took a screen record to present this issue:
!",0
636,I updated the issue title and desc to reflect the current status :),0
637,"Hi  
I noticed that there is a TF FloorMod reference implement here：


In my point of view，`floormod(x, y) = x - floor(x / y) * y` is  equivalent to the implementation of above, but not equivalent to ` (1/rhs * lhs) - floor(1/rhs * lhs)`, seems it missing a multiplier `rhs` 
i means   `rhs * ( (1/rhs * lhs) - floor(1/rhs * lhs) )`

Please correct me if I'm wrong.",2
638,"Hint, hint, call the thing VSterm?",0
639,"As a work-around, click on the search-results item twice.",0
640,This kind of documentation is not a trivial one liner.,0
641,"> device_map = {
        ""encoder"": {0: [0, 1, 2, 3, 4, 5] },
        ""decoder"": {1: [0, 1, 2, 3, 4, 5] },
    }

In this example, while the encoder is running GPU1 is idle.",0
642,Then `gradcheck`  should succeed with either `masked=True` or `masked=False`.,0
643,My overall goal: Weigh incorrect predictions more than correct predictions.,0
644,yes I could reproduce the issue with TF `2.11.0-rc2` but not with `2.9.1`.,2
645,It works well with RTX cards on TF 1.15.x and non-RTX cards on TF 2.2 (like nvidia T4).,0
646,We requested a change in cuDNN to try loading the local DSOs (from the PyPI wheels) and to load the system-wide libs as a fallback.,0
647,"So for some files there's essentially never a line highlight, even when stepping through results in the same file.",0
648,"Eg I would rather triple click expand selection, personally.",0
649,In your example you can set global seed using `tf.random.set_seed` and at Op level say for `shuffle()` you have `seed` argument where you can set some seed value here.,0
650,"> 
> Placing model shards on intended devices will also get easier with the Remote Module API (currently private [here]( cc   about GPU support for RPC

Thank you very much for this insight.",0
651,any updates on this when this will be resolved?,1
652,I also think that it makes for a great loss function too.,2
653,"Ok, unfortunately I am out of ideas.",1
654,The Keras guys wrote in  that it looks like a TF issue and that we should report it here.,2
655,Is this satisfactory?,1
656,I've developed and published syntax highlighting extension based on Tree-Sitter.,0
657,"** Next I run the container and go inside it:
`sudo docker run -it -v $PWD:/host_dir tf-builder bash`

**6.",0
658,Another approach might be to develop as you have for your example app (the extracted terminal) and slowly migrate the terminal code out into another library.,0
659,"Two follow-up questions then:
1.",0
660,Please see [this colab]( for the revised version.,0
661,The matched area is quite small on large screens.,0
662,How can I turn it off because it does something unexpected to me as a user?,1
663,"Also, I saw the [ documentation and saw that it is defined in the `padding.h` header file.",0
664,"> You are receiving this because you were mentioned.Message ID:
> ***@***.",0
665,"This should be done anyway I think, but I don't see this versus the other part of the argument.",2
666,"~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in wrap_layer_functions(layer, serialization_cache)
    202           if isinstance(fn, LayerCall):
    203             fn = fn.wrapped_call
--> 204           fn.get_concrete_function()
    205 
    206   # Restore overwritten functions and losses

/usr/lib/python3.8/contextlib.py in __exit__(self, type, value, traceback)
    118         if type is None:
    119             try:
--> 120                 next(self.gen)
    121             except StopIteration:
    122                 return False

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in tracing_scope()
    365       if training is not None:
    366         with K.deprecated_internal_learning_phase_scope(training):
--> 367           fn.get_concrete_function(*args, **kwargs)
    368       else:
    369         fn.get_concrete_function(*args, **kwargs)

~/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)
   1365       ValueError: if this object has not yet been called on concrete values.",0
667,Could you please check this issue?,2
668,"With the existing implementation it could just be offered under a ""Function Test Rollup"" header or something and aggregate the results together somehow, but when run it wouldn't update the ""referenced"" tests necessarily (though I guess I could do this within the context of the extension) so its fragmented and not ideal.",2
669,"maybe I should search for `bar` instead of `br` here"".",2
670,Can you confirm the architecture of your system using command `uname -m`.,2
671,Then **in a batch of all cats** (which is possible for small batch sizes) the **'effective weight' of cats will be 1 not 5**.,0
672,"For the forseeable future, things will be simplest if you provide a concrete class for the type natively in c10 (you might be able to hack out-of-line registration using TypeMeta but it seems hardly worth the effort).",0
673,"That's a performance penalty, even if functionally things work.",0
674,Search across files does rather less.,0
675,As far as I can tell FSDP runtime utils is doing the actual update step.,2
676,It's converting std.constant to hlo.const in this pass is what would be weird!,0
677,Copy the TFLite file you generated earlier to the path `android_repro\app\src\main\assets`.,0
678,"""Feature"" request.",0
679,"I would like to request documentation on the ideal build order for tensorflow and both its required dependencies as well as suggested dependencies:

As described in: 
the package `tensorflow` depends on 4 dependencies that must be moved in step:


However, it is my understanding that in order to build `estimator` that `tensorflow` must be importable by python.",2
680,"Seems like a nice-to-have but I'm not sure it's worth extending the DAP for it, unless we also use it to show decorations for column bp options like chrome devtools does, and I think the issue for that was closed.",1
681,Do we still want to revisit this using getPossibleBreakpoints?,1
682,"I'm not pinging your team asking for this to be resolved, i understand i don't drive your development plan.",0
683,"The only thing we (VS Code) could do is to add the notion of a ""terminal group"" to the [DAP request ""runInTerminal""]( that many debug adapters use to run something in the integrated terminal.",0
684,I installed from scratch so maybe that could help?,1
685,"It is not the solution to say ""it is no longer supported"".",0
686,"Btw, the title ""gradcheck produces false positives.."" may be misleading as the reported issue depends on the backward formula of `sampled_addmm` and the `gradcheck` numerical path does not use the backward formula at all.",0
687,"Editor note: This is #17139

----

 did you try after deleting `CMakeCache.txt` and `CMakeFiles` ?",0
688,- When respective target is zero (e.g.,0
689,So I am not 100% sure what is the issue here.,1
690,"Well, I tried many but nothing worked yet.",1
691,Yes,0
692,"In particular, it mentions that if there are two elements in the batch and the other element has a very different weight, then it will modify the gradient of the first sample.",0
693,"In your example you can set global seed using
> tf.random.set_seed and at Op level say for shuffle() you have seed
> argument where you can set some seed value here.",0
694,"Just to note, on StackOverflow, there is a desire to be able to clear the integrated terminal when starting debugging.",0
695,"Installed using `conda install pytorch torchvision cudatoolkit=10.2 -c pytorch ` from San Francisco, California and it's taking *ages*.",0
696,"The other simple alternative is a pair of functions such as `provideImplementationRanges(testItem: vscode.TestItem): ProviderResult<vscode.Range[]>` / `provideAssociatedTests(range: vscode.Range): ProviderResult<vscode.TestItem>`, which VS Code can call on demand when the user asks to go between tests and implementation.",0
697,**Edit**: Apologies for the wrong statement above.I would like to correct myself as tf-estimator is still under required packages for tf-nightly(2.13v.Dev) also.,0
698,I can take this up if there is interest in adding this to pytorch.,0
699,Back to the issue.,0
700,It might be hard to add that kind of logging directly into the PyTorch code since it may not be what all users want.,1
701,"One PhD student made the switch, but I will advocate for the rest of to do the same.",0
702,"The information I have parsed is below:
* Test name: `test_mem_leak (__main__.TestProfilerCUDA)`
* Platforms for which to skip the test: linux, rocm
* Disabled by `jithunnair-amd`
Within ~15 minutes, `test_mem_leak (__main__.TestProfilerCUDA)` will be disabled in PyTorch CI for these platforms: linux, rocm.",0
703,"Hey, I took a look at the the other bug report you mentioned.",0
704,I think a larger feature to decide is about supporting non-integer powers in matrix_power.,0
705,The pure pytorch example works fine and can back-prop gradients.,0
706,"**AUTO-PATCH WITH SHELL SCRIPT:**




_( Based on [StackOverflow]( answer by [Jeremy Caney]( / [LoveriusB]( )_",0
707,"### 🐛 Describe the bug
 

This fires the optimizer every time the gradient gets accumulated.",0
708,"Also, pylint gave me a lot of errors when I downloaded it.",0
709,I'm actually using `bun` instead of `npm` and it works fine too (just comment / uncomment the two lines accordingly),0
710,"I'm not sure whether the responsibility for this lies in VSCode or TypeScript, so if I should be moving this there, let me know :)",1
711,does it work on reload?,1
712,"But it is all related to the question of parametrization that we discussed in  I still believe that having `masked=True` is a bad design as it might create a false illusion of ""correct"" gradients, because the user could supply any `grad` into `autograd.grad`, and if this gradient is not properly projected, the resulting gradient might be incorrect, and we cannot detect such cases with `gradcheck` set to `masked=True`, only with `masked=False`.",2
713,"Now, if you think that you could submit a PR adding complex support for CPU, by all means go for it, but I don't think anyone in the inductor currently has the bandwidth to assist on this issue.",2
714,"**
  - To answer this, I would need to scan symbols for all files in the same directory as the test.",1
715,The weighted sum is coincidental.,0
716,It projects the input to handle arbitrary perturbations of `gradcheck` - check!,0
717,Could you please look into this issue?,2
718,"<img width=""924"" alt=""CleanShot 2022-12-02 at 16 52 34 (1)"" src=""
<img width=""940"" alt=""CleanShot 2022-12-02 at 16 52 30 (1)"" src=""

You can see a similar result play out here:
!",0
719,Nothing unexpected so far.,0
720,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type
Bug
### Source
source
### Tensorflow Version
2.9.1
### Custom Code
No
### OS Platform and Distribution
Linux Ubuntu 20.04
### Mobile device
_No response_
### Python version
3.9
### Bazel version
5.1
### GCC/Compiler version
9
### CUDA/cuDNN version
_No response_
### GPU model and memory
_No response_
### Current Behaviour?",0
721,"The `sampled_addmm` in non-masked semantics should be defined as

where `mask = (s != 0)`.",0
722,You'll be forced to spill to memory when you don't need to and use fake 0-d memrefs for scalars unnecessarily.,0
723,"""[c]"": {
        ""files.encoding"" : ""cp950""
    },
    ""[cpp]"": {
        ""files.encoding"" : ""cp950""
    }",0
724,Marking high priority to review in triage meeting for potential incorrect implementation,0
725,"[image](

and for the probability we take the class probabilities and for the weights we take the usual weights one take for imbalanced classes with probability p. So we want to compute the mean and standard deviation of the above quantity 

!",0
726,"The change to update the links on the website is in progress internally, just waiting on the final approval.",0
727,"I had same problem on first try on FF, but once I logged into GitHub once it worked fine.",2
728,"It feels like your IndexedDB is just broken at this point, for some reason.",1
729,tabs and undocking are definitely on the list of things I want to do in the near future.,0
730,It would be nice to see in-build **SSIM/MS-SSIM** function in pytorch.,0
731,So encoding settings per project are mandatory for me.,0
732,Please assign this issue to me - I'll add the missing rewrites needed to get this working.,0
733,cc,0
734,"I have not find out how to **quickly** search only the **current** file
!",0
735,"That said, is it possible that the current PyTorch behavior could be preferred for the class imbalance case?",1
736,Does the Chinese IME use ` specially?,1
737,"Hi ,

I was able to replicate your issue with the following steps:



It seems like that header (dnnl.hpp) is only included with some build_with_mkl flags, I tried a couple of different things but they all failed as well.",2
738,Could one of you please take a look at it?,2
739,Same problem on 98 for me (both with devtools open and closed),2
740,""".format(task))

OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function.",0
741,[snip_20161103092432](,0
742,"List the content of the bucket, looks like Windows is nowhere to be seen.",2
743,Would any of you be able to share that?,2
744,And so you're saying that we should do the same everywhere when we switch a whole set of operations to a new device.,0
745,Seems not to be a common problem.,0
746,"By
giving the seed value to shuffle based on the epoch, I can easily access
the data from that epoch.",0
747,Thus this pull request aims to add find all button in find widget.,0
748,"> They’re kinda different issues and this has been open for 2 years now so I’d rather leave this open until it gets fixed

any updates?",0
749,"Perhaps we could actually go with a ""pull-only"" model here, and have a user settings to configure the ""pull aggression.""",1
750,Thank you for your support.,0
751,The server is started by the context object.,0
752,"That's super-useful and needs be absolutely documented - thank you,  

So in the [mp tutorial]( the code needs to be changed to call `torch.cuda.set_device()` before switching to using the new device?",2
753,"> 
> > The conversion looks backwards intuitively.",0
754,"I can't speak for v2 but the last time I tried Hyper it was really slow & buggy, especially under Windows.",0
755,I can work for now with the .aar files but the reason I need to build for C++ is because I'm looking for cross platform integration.,0
756,Having a verified implementation inside pytorch would be very useful indeed.,0
757,I am assuming that you are open to the possibility of assigning weights not just because of class imbalance but also because of some classes being harder to train.,1
758,"There's a few reasons but they all stem from us wanting to eventually delete the Type class entirely (so that we can have open registration of operators, and more conveniently support use-cases where you only want a subset of operators to actually be registered.)",0
759,I think before we can do that we should revisit our current implementation and think about dropping in a better one that is just generally matching better.,2
760,期待新特性！,2
761,The fact that `ht` matches is due to our camel case matching.,0
762,> `sampled_addmm` does satisfy `func(densify(x)).to_dense() == func(x).to_dense()` holds for `x`.,0
763,"Hi  

As per the [documentation]( it should only be used if  we are using TF ops that may not be linked in by default with the TF ops that are provided when using the SELECT_TF_OPS path.",0
764,"Obviously if such a situation were to unfold, the shared component (not entirely dissimilar to VTE) should be jointly developed and shared with VScode.",0
765,"Let's call this (A):
~~~
def fit_model_a(model, dataset, normalize_fn):
    model.compile(loss='mse',
                    optimizer=tf.keras.optimizers.RMSprop(0.001),
                    metrics=['mse'])

    x_train = dataset.copy()
    y_train = x_train.pop('MPG')
    history = model.fit(
        normalize_fn(x_train), y_train,
        epochs=100, validation_split = 0.2, verbose=1)
~~~
Using my own training loop however exhibits drastically different behavior.",2
766,Ultimate support for static arrangements of terminals with nested dynamic grids (groups) as described here seems to me a possible ultimate aspiration and would be very powerful indeed.,2
767,"And if it is so you can quickly see how the code becomes quite hairy and error-prone, since while wrong device of the op args gets detected at op-running time, but wrong default device isn't being detected most of the time, except in some buggy ops.",0
768,It works!,0
769,"I have some examples of the behavior for different apps/platforms in 

My take on this is just a flag to allow us to focus VSCode when opening a context menu.",2
770,"Please feel free to request support or submit a pull request on PyTorch GitHub: 
ONNX: processing=StableDiffusionProcessingTxt2Img, pipeline=OnnxRawPipeline
*** Error completing request
*** Arguments: ('task(2vhzwlojaeegnc8)', 'flower', '', [], 20, 'DPM++ 2M Karras', 1, 1, 7, 1024, 1024, False, 0.7, 2, 'Latent', 0, 0, 0, 'Use same checkpoint', 'Use same sampler', '', '', [], <gradio.routes.Request object at 0x000001C4F60E7A00>, 0, False, '', 0.8, -1, False, -1, 0, 0, 0, False, False, 'positive', 'comma', 0, False, False, 'start', '', 1, '', [], 0, '', [], 0, '', [], True, False, False, False, 0, False) {}
    Traceback (most recent call last):
      File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\modules\call_queue.py"", line 57, in f
        res = list(func(*args, **kwargs))
      File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\modules\call_queue.py"", line 36, in f
        res = func(*args, **kwargs)
      File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\modules\txt2img.py"", line 55, in txt2img
        processed = processing.process_images(p)
      File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\modules\processing.py"", line 736, in process_images
        res = process_images_inner(p)
      File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\modules\processing.py"", line 841, in process_images_inner
        result = shared.sd_model(**kwargs)
    TypeError: 'OnnxRawPipeline' object is not callable

---'''

I already tried to downgrad torch to 2.0.0 but then i get a compatibility error for torch-directml.",0
771,It would be a lot easier if something like this landed in Electron  that would allow sharing all the services across multiple windows.,0
772,"`nvidia-smi`
Wed Nov 18 15:12:04 2020      


and next

`kill -9 125022`",0
773,For 4 minutes or so my 32GB RAM is full and my CPU (R7 7800X3D) utilization is also between 60% and 100%.,0
774,"I think this prints the number of individual batches where the updates have been performed, in your case if you are doing n iterations on your data then `optimizer.iterations` prints n.",2
775,a markdown file and a java file.,0
776,"That's why the PID is written in the lockfile, and why there's a ""debug anyway"" option on the dialog that js-debug presents.",0
777,"-  wants to be a user of this style of relations 🙂 
- Have an optional handler so that extensions can lazily serve related code only when explicitly asked for
    - If discovery of related code cannot reasonably be done in realtime, allow a provider containing the proposed `provideImplementations(testItem: vscode.TestItem): ProviderResult<vscode.Location[]>` / `provideAssociatedTests(location: vscode.Location): ProviderResult<vscode.TestItem[]>` methods to be added to the TestController.",0
778,"(yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem.",0
779,"If someone tells me which of these implementation decisions follow closer the current Pytorch guidelines, I'm happy to implement them myself and PR them",1
780,conda?,2
781,Can this be re-visited please?,0
782,Do we agree that `gradcheck` producing `True` with `masked=False` is incorrect?,0
783,On MacOS 10.15 when a file is open from an SMB share if connection t othe SMB share is lost the entire editor crashes.,0
784,"If I swapped out the GT 1030 with an RTX A2000, it would install cudnn version 8.6.0.163 without issue.",0
785,"Alternatively, you can [build extension from sources](",0
786,On windows I get this error during setup when it starts to get PyYAML dependency,0
787,"I think such changes are very likely to be rejected by the autograd team (the complexity of the needed changes is much higher than that of  which was [rejected]( as too complex), so I find the discussion about removing the `densify` step without the support from the autograd team just too unrealistic.",2
788,1,0
789,"After running the app, when you change the Delegate to `GPU`  and click the `Run` button, you should be able to see the error message in Android Studio's Logcat.",0
790,I've never seen this before.,2
791,Tried running the test_torch.py file but it just says it failed to import torch.,2
792,This ensure numerical stability for the important sample during the backward pass.,0
793,"Not as far as I know

>  can you check if  works for you and if not attach the full output of devtools console?",2
794,(You don't have to change MainViewModel code.),0
795,"Ok, I think I found something useful, let me describe the video a bit:
1.",2
796,"This is all I see in Safari's console:
<img width=""753"" alt=""Screen Shot 2022-03-23 at 9 03 12 AM"" src=""

I do not see any network errors.",2
797,In particular I think that solving #10546 would minimally solve a lot of asks simultaneously in a generic way.,2
798,"I believe adding `""noErrorTruncation"": true,` to your `tsconfig.json` has the side effect of expanding `... n more ...` in your case.",2
799,"While, while we're developing on it, tends to happen.",0
800,"When I try to fit the model with a small batch size, it successfully runs.",0
801,"System information

- OS Platform and Distribution: TFLite conversion on Windows 10 and run models on Android 13
- TensorFlow installation: pip package
- TensorFlow library: 2.14.0

### 2.",0
802,"For that hook signature, there is no return value.",0
803,"I've copied and modified the build_aar.sh file:
[build_tf_flex.sh](

And divided my previous BUILD file into to different files:
[BUILD_tfliteops](
[BUILD_tfops](

I also deleted lines _52 to 60_ (that added oneapi source files) from my Dockerfile:
[Dockerfile](

The _tensorflowlite.so_ file is successfully created, but I get a lot of undefined reference errors when building the _tensorflowlite_flex.so_ file:
[stderr-2](

This is the command I'm using from the Docker container to build:
`bash custom_files/build_tf_flex.sh --input_models=custom_files/cpp_tf_test.tflite --target_archs=arm64-v8a` Not sure if this is the correct target when building .so files for android, since other guides say you should use _android_arm64_.",0
804,That's what I'm doing for the time being and it works fine.,0
805,I think this is probably an ABI compatibility problem.,0
806,"Any updates on the development of `expm`, `logm`, `sqrtm`?",1
807,I have been able to create my own working version of SSIM and MS-SSIM thanks to you.,0
808,"However, I am not using .function at all in model (B), where the problem occurs.",0
809,"> 
> Right.",0
810,The padded format (where all stirings in array occupy same number of bytes) can be just a regular dense tensor.,0
811,Search only works for string without accented characters.,0
812,"-->
<!-- 📣 Issues caused by an extension need to be reported directly to the extension publisher.",0
813,"Suppose 
`x = torch.zeros(2, 2); x[0, 0] = 1; x = x.to_sparse()`, and
`p = torch.zeros(2, 2); x[-1, -1] = eps; p = p.to_sparse()`, then

`f(x)[-1, -1] = 0, f(x + p)[-1, -1] = 6 + eps`, so I guess the entry for `J[f_11,x_11](x) -> inf`.",0
814,PyTorch 2.1.0 works properly in this environment.,0
815,"They thought they could reserve a port, but since both of them are using SO_REUSEPORT, neither of them will think they fail.",0
816,I guess we must not be relaying out something when you toggle the panel.,1
817,"(2) is interesting, how would you envision the UI/UX for that?",1
818,"Tried it anyway, did not work.",1
819,This may be years.,0
820,Hard-force the match to top line of screen.,0
821,That is an omission as every user of std will run into this when lowering tensors to memrefs.,0
822,I confirm !,0
823,Not sure but it may be due to compatibility issue between GT 1030 and CUDA or cuDNN version or GT1030 GPU might not be supported.,1
824,"Platforms: rocm



This test was disabled because it is failing on MI210 runners as part of 


cc     -amd",0
825,So which approach are we going forward now?,1
826,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Tensorflow_probability.__version__:  '0.13.0'
- Python version: 3.8.10
- CUDA/cuDNN version: cuda_11.2.r11.2/compiler.29373293_0 
- GPU model and memory: 12Gb TitanXP

**Describe the current behavior**
Tensorflow model with tensorflow_probability layers creates errors while saving using 

** The model is created using the below code**
`
_model = Sequential([
        Conv2D(8, 5, activation='relu', padding='valid', input_shape=input_shape),
        MaxPooling2D(6),
        Flatten(),
        Dense(10),
        tfpl.OneHotCategorical(10)
    ])
    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

probabilistic_model = get_probabilistic_model(
    input_shape=(28, 28, 1), 
    loss=nll, 
    optimizer=RMSprop(), 
    metrics=['accuracy']

probabilistic_model.fit(x_train, y_train_oh, epochs=5)

_
`

!",0
827,I'm currently trying to run a TFLite model on an Android mobile device using the GPU Delegate.,0
828,"At any rate this is definitely a ""nice to have"" as opposed to a ""must have"" and could come at a later iteration, just a thought that would make testing really simple for people and not have to lose the context of where they are directly working on the application.",0
829,"[Screenshot 2021-07-06 at 22 44 42](

whereas the correct way (and the way Tensorflow does it) is

<img width=""173"" alt=""Screenshot 2021-07-06 at 22 47 40"" src=""


This issue is explained in more detail in 



## To Reproduce

Steps to reproduce the behavior:

1.",2
830,"> However, the only way I can then release the GPU memory is to restart my computer.",2
831,Probably repeating some of the above observations.,2
832,"Primarily to ease the facilitation of model parallelism, but it may have other uses.",0
833,"cc , if he differs in opinions from me :)",0
834,Check out the instructions in the README,0
835,I have compiled open-mpi with cuda support from source (4.0.0) but the error still occurred.,2
836,"Making `tf-mlir-translate -hlo-to-mlir-hlo` pipeline generate xla_hlo.constants would also make more sense, I think.",2
837,Are you satisfied with the resolution of your issue?,0
838,"Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem.",0
839,ah good point about `unweighted_mean`.,0
840,"(right clicking does not take focus)

(i.e.",0
841,Can you please follow the steps in  to get at more details around the crash and attach the result here?,2
842,+1 for this feature - I'm trying to adopt VS Code and the current state of this feature is a paper-cut,0
843,"Constructing entire syntax tree, Tree-sitter efficiently overcomes all limitations of built-in TextMate grammars.",0
844,"I'm away from my PC right now, but I'll check with an emulator on my end later to see if it's same as you.",1
845,"If you can show us the converted state after each time, that'll also be great.",2
846,"First, I need to find the line in question (vertical).",0
847,"So now after the model is initialized - we turn MP on by doing `.to()` to all the layers (and their sub-modules) according to the device map and then in `forward` we switch inputs to the same device as the layer's params using a handy wrapper I shared here: 
with a little bit of fiddling with some of the params which happen to be shared, this is all that needs to be done to make the model parallel.",0
848,A new pass std to hlo just for this?!,2
849,"Huh, I see what my issue is here:

I technically have 2 GPU's in my machine, and Tensorflow may be seeing the wrong one.",2
850,"original link for pip install torchvision was ""

the link I'm downloading it now; 

this is seriously weird.",0
851,Is there some news about this support function?,1
852,"Hi  ,

First you can use the API tf.config.list_physical_devices('GPU') to list out the physical devices.Then If you want to set only specific GPU visible to Tensorflow then you can use the API `tf.config.set_visible_devices()`

Below is example code for ignoring 1st GPU and making remaining all visible to Tensorflow.",0
853,- Tree-sitter (used in Atom and proposed in #50140) could provide both a declarative and higher-level imperative API to the lower-level semantics token provider API.,0
854,"bug_report_template ⚠️⚠️ -->
<!-- Please read our Rules of Conduct:  -->
<!-- 🕮 Read our guide about submitting issues:  -->
<!-- 🔎 Search existing issues to avoid creating duplicates.",0
855,I am having this issue as well.,2
856,Thanks for your help.,0
857,I did include tree-sitter in my post around VSCode performance as a whole,0
858,Since this thread hasn't been spammed with similar issues I assume there's a standard way to work around it.,0
859,"For cell editor bg, I think at one point we did use a bg color and then went away from that because of the accessibility issues it introduced with color contrast?",2
860,"By the way, what are you looking to log?",1
861,When I upgrade torch to 2.2.1 then I get several version errors refering to torch-directml incompatability.,2
862,"Let's keep the discussion on the original poster's issue, which is about Firefox.",0
863,"(Though on large screens, you need a cue for center.)",0
864,"After running `python setup.py develop`, I get this error:

Probably because I don't have cuda, however I cannot seem to disable this.",2
865,"My perspective — while such CPU simulation libraries can be useful for hardware design (and verification), they are practically useless for any software (training/inference) purposes, for which, at the least, we'd want GPU accelerated simulation.",0
866,"From feedback, I think one approach we can take is, like we do on tests, something of a hybrid:

- Have `relatedCode?",2
867,"Documentation including ""patches"" to apply to `tensorflow` and dependencies such as `tensorflow-io` to get the system to compile from a boostrapped environment.",0
868,"I have also been battling with the issue of releasing GPU memory for quite some time...

My use case is a machine in a production environment with a single Python process that has to serve different types of clients and I need to switch models depending on the service to be provided.",0
869,Shall we close this?,2
870,",
Please feel free to submit a PR for the requested change or share the link where requested change is to be made",0
871,"user data falls back to in-memory fsp in case indexedDB is not accessible or having errors, may be the storage service shall do the same?",0
872,can you check if  works for you and if not attach the full output of devtools console?,2
873,"52     objects, functions = (
---> 53         super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(
     54             serialization_cache))
     55     functions['_default_save_signature'] = default_signature

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)
     97     """"""Returns dictionary of serialized attributes.""""""",0
874,"Yes, but scrollbar is not useable anymore after hiding and showing panel, even when hovering terminal.",0
875,"Note that the conversion succeeds intermittently when converting the same network many times, but on average it fails.",0
876,Did you expect something else?,1
877,"]])>, b8=<tf.Tensor: shape=(2, 8), dtype=int64, numpy=
array([[8, 8, 8, 8, 8, 8, 8, 8],
       [8, 8, 8, 8, 8, 8, 8, 8]])>, b9=<tf.Tensor: shape=(2, 9), dtype=uint8, numpy=
array([[9, 9, 9, 9, 9, 9, 9, 9, 9],
       [9, 9, 9, 9, 9, 9, 9, 9, 9]], dtype=uint8)>, b10=<tf.Tensor: shape=(2, 10), dtype=uint16, numpy=
array([[10, 10, 10, 10, 10, 10, 10, 10, 10, 10],
       [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]], dtype=uint16)>, a=A(a1=<tf.Tensor: shape=(2, 11), dtype=float16, numpy=
array([[11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.",0
878,> The conversion looks backwards intuitively.,0
879,"> 
> I'm actually using `bun` instead of `npm` and it works fine too (just comment / uncomment the two lines accordingly)
> 
> 

It works with tasks.json, but don't with launch.json in configurations and compounds.",0
880,"Aren't we in control of what an IR generator can ""choose"" to generate?",1
881,"In conclusion being silent about this hurts:

1. performance of arguably 1 of the primary functionalities of Code.",0
882,Showing the inline instruction pointer also for run to cursor would be a nice feature.,0
883,Build `tensorflow`.,0
884,Did you try to upgrade the numpy version by using` pip install numpy==1.23.4`.,2
885,I think the main objection would be that it won't support older pytorch.,2
886,"Also, in order for our theming support to work well, we want all tokenizers to emit TextMate tokens.",0
887,Named pipes on Windows are created in a global namespace so we create the name by [hashing information]( about the VS Code version.,0
888,"[See CircleCI build]( [pytorch_xla_linux_bionic_py3_6_clang9_test]( (1/1)
**Step:** ""Run tests"" ([full log]( | [diagnosis details]( | [:repeat: rerun](
<details>
<summary>
<code>May 12 09:20:37 [  FAILED  ] AtenXlaTensorTest.TestPinverse</code>
</summary>
</details>
---
### XLA failure
Job **pytorch_xla_linux_bionic_py3_6_clang9_test** is failing.",0
889,I would simply multiply the inputs by a big number.,0
890,"I made a model that had two times fewer parameters, tensorflow still took up 31 out of 32 gigabytes.",0
891,"The latter — 

Alternatively, what you may want is more native support as opposed to a cpp extension, the way `c10::half` is on the CPU.",2
892,"I get this pretty frequently recently, !",0
893,"## Pitch

Add a new method that pads an array `[1,2,3,4,5,6]` symmetically by repeating the boundary pixel  as `[2,1,1,2,3,4,5,6,6,5]` (unlike `ReflectionPadNd()` that does *not* repeat the boundary pixel: `[3,2,1,2,3,4,5,6,5,4]`).",0
894,I can reproduce on a clean profile at the first load on Firefox Nightly 101.,0
895,"Came here to suggest splitting `'mean'` into:

* `'ordinary_mean'`: After all it is an ordinary mean of the weighted sum.",0
896,Current best hack-y workaround,0
897,"Can pytorch assert when such situation happens, so no such bugs are masked?",1
898,">
> You can save dataset iterators into checkpoints
> <
> so that they can be restored to the exact state.",0
899,"I have found that it is rather inconvenient to use the SSIM in skimage for Pytorch Tensors since they are in NCHW order, instead of the HWC order that skimage expects (same for MS-SSIM in skvideo).",0
900,"[image](

In the search above, I should have more than 1000 results ... and I only have 4!",2
901,"Hence we can expect `gradcheck` to work with `masked=True`:



However, the situation is reversed for `masked=False`.",0
902,"cc: -roy    

 things will likely change fast, see PRs by -roy for context such as 
cc",0
903,Any progress?,1
904,[You can install it from VSCode Marketplace.,0
905,> Is it as easy as checking for \\ in the path beginning?,1
906,"As an example, let's consider the `sampled_addmm` method which is semantically equivalent to
`sampled_addmm(s, m1, m2, alpha, beta) := alpha * (m1 @ m2).sparse_mask(s) + beta * s`.",0
907,🤣 this is why we prefer pull instead of push.,0
908,"🤷‍♀️
!",0
909,See #17139.,0
910,We also change the background color of the selected match.,0
911,"> Even as a proof of concept, tmux would make this already very useful for me!",0
912,"I'm too ignorant to give an answer, but a font manager is definitely what is needed.",0
913,"As you know, memrefs aren't necessarily contiguous and can have arbitrary affine layouts, striding, padding, and so an op that stores a constant to a memref just cannot live in the std dialect!",0
914,I was curious how logging is related to gradient clipping?,1
915,Further information on details of the code I cannot provide.,2
916,Maybe a similar solution could be applied to Ubuntu as well.,1
917,With CollectiveAllReduce strategy (aka MultiWorkerMirroredStrategy) it is an issue because it seems not possible to start the server upfront anymore ( can you confirm this statement is true ?),1
918,I will reinstate that.,0
919,We do not show the inline instruction pointer when the stop event has a column.,0
920,"Yes, something like that!",0
921,"We currently support the following platforms: asan, dynamo, inductor, linux, mac, macos, rocm, slow, win, windows.</body><!-- validation-comment-end -->",0
922,"Hi, just to reiterate, the quantization sometimes works already if one just tries it enough times, so this doesn't seem to require a new feature, unless the fact that the quantization sometimes works is the bug here.",0
923,Looking at alternatives.,0
924,It's still listed under `REQUIRED_PACKAGES` for [2.12]( and on the master branch.,0
925,> This is blocking the TensorFlow Rust release.,0
926,"Additionally, I would also want to know how to do it on `Keras` datasets which are used in `model.fit()`




### Standalone code to reproduce the issue




### Relevant log output

_No response_</details>",0
927,"- Ubuntu 18.04
- installed from source (with pip)
- tensorflow version v2.1.0-rc2-17-ge5bf8de
- 3.6
- CUDA 10.1
- Tesla V100, 32GB RAM

I created a model, nothing especially fancy in it.",0
928,Thanks a lot for all your inputs   -roy  .,0
929,"Back in this August, there was no such issue",0
930,I don't have torch-directml installed in my case.,0
931,Same error here from Brazil!,2
932,If I saw `'truemean'` I would be confused/think something fancy was going on.,0
933,"671         with OptionalXlaContext(compile_with_xla):
--> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)
    597       with autocast_variable.enable_auto_cast_variables(
    598           layer._compute_dtype_object):  # pylint: disable=protected-access
--> 599         ret = method(*args, **kwargs)
    600     _restore_layer_losses(original_losses)
    601     return ret

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)
    163       return wrapped_call(*args, **kwargs)
    164 
--> 165     return control_flow_util.smart_cond(
    166         training, lambda: replace_training_and_call(True),
    167         lambda: replace_training_and_call(False))

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)
    107     return control_flow_ops.cond(
    108         pred, true_fn=true_fn, false_fn=false_fn, name=name)
--> 109   return smart_module.smart_cond(
    110       pred, true_fn=true_fn, false_fn=false_fn, name=name)
    111 

~/tf2/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     52   if pred_value is not None:
     53     if pred_value:
---> 54       return true_fn()
     55     else:
     56       return false_fn()

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in <lambda>()
    164 
    165     return control_flow_util.smart_cond(
--> 166         training, lambda: replace_training_and_call(True),
    167         lambda: replace_training_and_call(False))
    168 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)
    161     def replace_training_and_call(training):
    162       set_training_arg(training, training_arg_index, args, kwargs)
--> 163       return wrapped_call(*args, **kwargs)
    164 
    165     return control_flow_util.smart_cond(

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in call(inputs, *args, **kwargs)
    679     return layer.keras_api.__call__  # pylint: disable=protected-access
    680   def call(inputs, *args, **kwargs):
--> 681     return call_and_return_conditional_losses(inputs, *args, **kwargs)[0]
    682   return _create_call_fn_decorator(layer, call)
    683 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in __call__(self, *args, **kwargs)
    637   def __call__(self, *args, **kwargs):
    638     self._maybe_trace(args, kwargs)
--> 639     return self.wrapped_call(*args, **kwargs)
    640 
    641   def get_concrete_function(self, *args, **kwargs):

~/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--> 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

~/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    922       # In this case we have not created variables on the first call.",0
934,(HF `transformers`).,0
935,"As a user, you should not be concerned about the `densify`, it is an internal method.",0
936,<br> `Stale` pull requests will automatically be closed 30 days after being marked `Stale` <br>,0
937,"I agree that this implementation would make things easier, but do you know whether you can use this with FSDP?",1
938,"For instance, `int4` type is another tensor type in which I am interested.",0
939,[grafik](,0
940,"VSTerminal as a standalone app would be excellent 

I've used Hyper.Js and Terminus, both on windows, specially Hyper has a lot of bugs, it didn't work properly for me and it was slow,  VS Terminal works like charm, very configurable , is fast, and packed with some of the Hyper feature

Current Features: 

- Open Multiple Terminal in ""tab/dropdown""
- Open Different Terminal (cmd, powershell, bash, etc)
- Split functionality
- Set Custom terminal from settings (terminal, arguments, font, fontsize), just to name a few, i was able to set Cmdr
- Fast!",0
941,> > The original issue was of making the -hlo to lhlo pass deal with this input.,0
942,"Same here from Malaysia, just want to download pretrained ResNet.",2
943,"Hi, can you explain this in more details?",1
944,"Before with PS strategy we were able to start the tf server on our own:


and then just injecting 'google' env in here to prevent the server from starting again:


This reduced the race condition of port reservation and therefore we never really saw the problem.",0
945,", do we upload the pip binaries also to download.pytorch.org?",1
946,This group concept can be seen as *completely independent of and complementary to* the useful idea of a static UI arrangement of terminals which persists between sessions of VS Code and which could ultimately be 3D (i.e.,0
947,Something more or less advanced?,2
948,"> 
> 
> 
> I was curious how logging is related to gradient clipping?",1
949,"***>
>",0
950,I agree this feature is necessary.,0
951,This was discussed in  and the RFCs from Tencent/Alibaba are under discussion  .,0
952,: vscode.Location[]` on the `TestItem`?,2
953,Could you propose a fix to the backward formula of `torch.sparse.sampled_addmm` for the non-masked semantics case?,2
954,It's really weird.,0
955,"Would you expect a specific test to run, or just the test file that was found and believed to be associated with the code?",1
956,"For example, if the user right clicks on the test item or the play button in the gutter, the context menu has ""Go to related code"", which either jumps or opens a peek view like ""Go to definition""; and ""Find all related code"", which opens a side bar view like ""Find all references"".",0
957,"- However, to fully answer this question I would need to resolve all tests in the directory to ensure I didn't miss anything.",1
958,"Hi -wang-g, assigning this to you to consolidate tflite-model-maker issues.",0
959,> did you configure anything in your settings that could prevent IndexedDB from being opened or created or maybe some quota limit is hit.,1
960,perhaps calling it `reduction='unweighted_mean'` would be more informative?,2
961,If there was an option for extensions to enable fuzzy search this kind of reasoning would become even messier.,0
962,love to have this feature,0
963,"Hi  ,

With latest version tf-estimator was deprecated and no longer needed for build and refer the [source](

GCS filesystem is built by SIG IO community as communicated in [comment-1031094136] (

Could you please confirm whether you still working on this issue ?",0
964,"Hi  

Did you get a chance to run the build?",2
965,Resolving the issue because the test is not flaky anymore after 700 reruns without any failures and the issue hasn't been updated in 14 days.,0
966,"If my desired global gradient clipping norm is 10.0 then I could
just clip each parameter to 10.0/sqrt(len(parameters)).",0
967,### 5.,0
968,I had the same issue,2
969,Place an underline or box around the emphasized line.,0
970,I guess this can be shared among devices?,1
971,"Same issue here, even in azureml public docker.",2
972,"Fwiw, finder allows you to edit file names even without focus.",0
973,:),0
974,THanks Uday!,0
975,"I'm not super keen on this since there is complexity around dealing with invalidation and re-pulling, but the path is there.",0
976,"Double-click selects a word, determined by ""editor.wordSeparators"".",0
977,_Originally posted by  in,0
978,I tried your fix and I'm still running into same error,1
979,> Could you propose a fix to the backward formula of `torch.sparse.sampled_addmm` for the non-masked semantics case?,2
980,"Therefore, it is reasonable for `vscode-go` to assert that `ExampleExecuteFeature` is an example of `ExecuteFeature` and `TestExecuteFeature` is a test of `ExecuteFeature` and therefore when `ExecuteFeature` is modified, the example and test should be executed.",2
981,Are you still testing this on Firefox or a different browser?,1
982,"M1 related build issues are officially not supported by Tensorflow, since we currently don't have Tensorflow build for M1 officially from us.",0
983,The paper I uploaded illustrates the effects of SSIM with different window sizes very nicely.,0
984,"In Monarch you could shift states to an explicit state, and then ""pop"".",0
985,"Hence, using `masked=False` is invalid usage and the `gradcheck` result is undefined (there is no way for `gradcheck` to determine if the input function and the specified `masked` option are consistent).",0
986,"Regarding the data size you pointed out, I have confirmed that the batch size is fixed to 1 during TFLite conversion.",0
987,"Nevertheless, we should probably make sure that the page comes up even when no indexed DB connection is possible.",0
988,The densification of sparse inputs allowed more-or-less transparent addition of sparse tensors support to autograd for non-masked semantics.,0
989,"Hi, I just solved this issue.",0
990,"For instance, if I were to experiment between two alternative implementations of the type — one using `uint16_t` as in my current write-up and the other using a quantized `float`, is there a way to start off with an interface of a base class definition for `bfloat16` and supply an overloaded definition as part of the extension?",1
991,"_From  on March 16, 2017 9:57_
_Copied from original issue: Microsoft/vscode-docs#889_",0
992,1.,0
993,"> 
> I don't see the reasoning here.",1
994,"]],
      dtype=float16)>))], which did not match Tout=[B.Spec(b1=TensorSpec(shape=(2, 1), dtype=tf.float32, name=None), b2=TensorSpec(shape=(2, 2), dtype=tf.float16, name=None), b3=TensorSpec(shape=(2, 3), dtype=tf.int8, name=None), b4=TensorSpec(shape=(2, 4), dtype=tf.int16, name=None), b5=TensorSpec(shape=(2, 5), dtype=tf.int32, name=None), b6=TensorSpec(shape=(2, 6), dtype=tf.bfloat16, name=None), b7=TensorSpec(shape=(2, 7), dtype=tf.float64, name=None), b8=TensorSpec(shape=(2, 8), dtype=tf.int64, name=None), b9=TensorSpec(shape=(2, 9), dtype=tf.uint8, name=None), b10=TensorSpec(shape=(2, 10), dtype=tf.uint16, name=None), a=A.Spec(a1=TensorSpec(shape=(2, 11), dtype=tf.float16, name=None)))].",0
995,"Or have them use the tree sitter API wrapped in a service (for queries etc)

If anyone is interesting in helping there's a PR here:",0
996,"I tried that, but it only gives results from the files that I've opened.",2
997,"Thanks for getting back to me  :blush:
> You can use torch.amax() that does support multiple dims though:

That's true.",0
998,"Coming from TensorFlow, I would expect we legalize to HLO (without standard in the mix), including the nested regions of reductions.",0
999,"We also have PP in `fairscale` and `deepspeed` which we are now being integrating into the HF trainer, perhaps we could use those if there will be an objection to requiring pt-1.8, since it might be easier to ask for a specific version of the 3rd-party module if someone wants to use MP/PP.",0
1000,This dataframe could be scattered without copies to DataLoader workers.,0
1001,Any issues with numerical stability  pointed out should be taken care at the level of learning rate itself and not come through this implementation of class weights.,0
1002,hi is it still worked on?,1
1003,Most of the challenges I'm having with #45667 are codegen related.,2
1004,"],
       [2., 2., 2., 2., 2., 2.",0
1005,"However, when we want to learn something from user's behavior, large embedding layers are always required.",0
1006,This case is the hardest for the user because the user should be really careful with the gradients and data manipulations.,0
1007,I did what was done in the video and things do seem to run okay.,0
1008,"If `p = eps * x`, so it shares the sparsity pattern of `x`, then both paths would be finite and match (=1), this is what happens
when `densify` is used in the numeric path.",0
1009,"While the converted TFLite model works perfectly on my PC, I encounter an error when trying to load it on the mobile device.",0
1010,That was numeric path.,0
1011,"Editor note: This is a different issue, see #7065

----

I experience the same error on MacOSX 10.13.",0
1012,The usage of `densify` is a problem since I cannot test what I want with `masked=False`.,0
1013,The original example was meant to represent a real world example where someone saves a model and then continues training.,0
1014,`weight_normalized_mean` could then be an option for the current version (if you ever were to rename it).,0
1015,I already confirmed this with you via ``nvidia-smi`` and ``uname -r``.,0
1016,I've experienced the broken heuristic in mono-debug with multi threaded programs.,0
1017,Check that your missing symbol actually is the same as the one here.,0
1018,"`nvidia-smi` command is not related to Tensorflow.It outputs Driver version, CUDA toolkit version it is compatible and also GPUs on local machine.",0
1019,"2023-05-23 14:23:20.166114: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.",2
1020,", Thanks for working on SSIM.",0
1021,"I'm getting something very similar in TF 2.11, on Mac M1.",2
1022,Just in case you need a taste for the limitations of various matrix exponential algorithms:,0
1023,Sadly not.,0
1024,> I am not sure that such a fix will cut it.,1
1025,"This is my new .sh file:
[build_tf_flex.sh](

It is back to the error related to missing dnnl files:
[error_logs](",0
1026,Thanks.,0
1027,"We could default to searching it as utf8 which would work for a-z at least, if that helps.",0
1028,"Please verify that the platforms list includes all of
            [linux].",0
1029,> Is there a way to unregister a new scalar type after registering it like so here?,1
1030,I think `Location[]` is a good first step.,2
1031,"There will be some differences such as configuration and some things are handled differently (clear in Hyper may not work on Windows for example), overall it will be very close to VS Code though.",0
1032,Add a Standard to HLO conversion so that you dont replicate code (this is a one line using DRR).,0
1033,"We can have two tf jobs starting at the same time, yes, but the initial port assignment is random, so the probability that this collides is really small.",0
1034,It's not working for me on refresh nor after deleting all locally stored website data in Safari.,1
1035,"],
       [11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.",0
1036,"I am getting exactly the same warning as  and , and I am running my code on an Ubuntu 22.04 server, i.e., the issue is definitely not limited to (M1) Macs.",1
1037,Can you please check [this issue]( which is very similar to you but a classification problem.,0
1038,"But I dont see any reason not to have such a pattern, but maybe you could reuse some of the conversion from hlo -> lhlo for std -> lhlo conversion.",2
1039,> THanks Uday!,0
1040,Definitely agree that `reduction='truemean'` is not a great name.,0
1041,What is the default device in that example?,1
1042,You are forever switching between terminals (whose order is a bit random).,0
1043,> dealing with files as a means to lock a folder is complicated.,0
1044,Pressing the shortcut key means you don't have to use the mouse.,0
1045,"As a data point, 's  implements a weighted cross entropy by multiplying the unreduced loss by weight and then reducing with a mean()",0
1046,Still experiencing the same error.,2
1047,"Hi,
Our max() function is very overloaded and doesn't have this feature right now indeed.",0
1048,"I did not touch that line; the left pan just uses the standard encoding UTF-8 when reading the file from history; not the one I set in setting.json which reads



!",0
1049,"I think the ""filtering"" and ""ranking"" logic should be considered separately.",0
1050,"* Master creates configuration and broadcasts TF_CONFIG variable to all executors
* Launch tensorflow servers

**Any Other info.",0
1051,This could be similar to the search functionality in [Textmate]( where you have the option to see all occurrence as an output in a single page and you can then do extra actions like copying those returned occurrence as rawtext.,2
1052,"I work on image restoration and GAN, so I used this metric a lot.",0
1053,-varma thoughts?,0
1054,I didn't see this kind error in tf 2.8 with the identical code.,0
1055,So far it loads up fine but there's some issues having it properly instantiate tree-sitter.,0
1056,"I tried it out and got following error:

Traceback (most recent call last):
  File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\launch.py"", line 48, in <module>
    main()
  File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\launch.py"", line 39, in main
    prepare_environment()
  File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\modules\launch_utils.py"", line 618, in prepare_environment
    from modules.onnx_impl import initialize_olive
  File ""C:\Users\Niklas\pinokio\api\automatic1111.git\app\modules\onnx_impl\__init__.py"", line 17, in <module>
    class DynamicSessionOptions(ort.SessionOptions):
AttributeError: module 'onnxruntime' has no attribute 'SessionOptions'
Drücken Sie eine beliebige Taste . . .",2
1057,"So, to be clear, the agreed upon plan for CUDA complex support is that we are going to we are going to replace native complex tensors with a subclass that keeps real and imag as separate tensors  and then use  tensor subclass compilation to compile it into native code without any ""extra"" complex support for inductor.",0
1058,The same warning always occur.,0
1059,"The benefit of the `#2` is clear when using dual-screen: tests can be run on another screen while keeping the code with another small terminal in the bottom, **WITHOUT loosing the ability to `CMD-click` on failing tests' paths** in the detached terminal running the tests, to directly open their editor in the main VSCode window.",0
1060,"> 
>  Could you please look into this issue?",2
1061,"Here is what I end up with:


**Describe the expected behavior**

The TensorFlow library to be imported without issue.",2
1062,Any chance there is a method to manually specify just some ops from the tf_ops set to be included in the build?,1
1063,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type
Bug
### Source
binary
### Tensorflow Version
2.9.1
### Custom Code
No
### OS Platform and Distribution
Linux Ubuntu 20.04
### Mobile device
_No response_
### Python version
3.8
### Bazel version
_No response_
### GCC/Compiler version
_No response_
### CUDA/cuDNN version
_No response_
### GPU model and memory
_No response_
### Current Behaviour?",0
1064,This breaks the fundamental assumption in gradcheck and requires considerable changes to the gradcheck numerical path.,0
1065,Has anyone solved it?,1
1066,I have no idea how may of sparse-supporting functions actually produce wrong gradients.,1
1067,bump this is still a problem,2
1068,"The order of the approximation does not matter, really, it only applies to differentiable functions.",0
1069,Can developers please tell us that by when we should expect the implementations of expm and logm functions in pytorch?,1
1070,"For that hook signature,
> there is no return value.",0
1071,> performance of arguably 1 of the primary functionalities of Code.,0
1072,> Suppose `x` is a sparse 1D tensor with `nnz` elements and `x.to_dense()` is an `n`-dim vector.,0
1073,"So we can try to reproduce this issue

I'm using a deeplearning image on GCP, `projects/deeplearning-platform-release/global/images/common-cu121-v20231105-debian-11-py310`",0
1074,So now we get an automatic switch of all inputs to the same device as the params of the model.,0
1075,"Instead of using clang (I'm on macOS) to compile pytorch directly, use mpicc/mpicxx and change the wrapper compiler to clang:
sudo MACOSX_DEPLOYMENT_TARGET=10.9 CC=mpicc CXX=mpicxx OMPI_CXX=clang++ OMPI_MPICC=clang python setup.py install

Successfully compiled on macOS 10.13 / CUDA 10.",0
1076,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type
Support
### Have you reproduced the bug with TF nightly?",0
1077,That worked.,0
1078,The latest version of CuDNN is version 12.,0
1079,"I understand it might be challenging, but I'd like to explore the possibilities and see if I'd be able to accomplish it.",0
1080,"It's possible to set device to `1` and then operate on the tensors on device `0`, but for every function internally pytorch would be calling `cudaSetDevice(0)` - launch function kernel - `cudaSetDevice(1)` as part of setting device guards, and this is generally less efficient then setting device to `0` in the first place.",0
1081,My only option at this point is downgrading the CUDA version.,0
1082,I followed all of the instructions in the guide.,0
1083,"> In this case, just like torch.stack([param grads], dim=0).norm(2, dim=0) would be our overall grad norm.",0
1084,Thanks for looking into this.,0
1085,> I think it would be tough to enable this for all pickers so we might need an option here for the extension to enable this.,1
1086,Does this issue occur when all extensions are disabled?,1
1087,> `uname -m` Returns the same architecture as you.,0
1088,"Currently the idea is to put the 6 encoder layers on gpu 0 and the same for decoder layers but on gpu 1:

or alternatively, splice each group as following:

and the remaining non-encoder/decoder layer modules can be all on gpu 0 or grouped closer to where they are needed.",0
1089,Kindly find the gist of it [here]( Thank you!,0
1090,Sorry about that.,0
1091,Find all button in other major editors comes with find widget itself.,0
1092,(hopefully not...),0
1093,The warning message:,0
1094,"I've been getting lots of errors saying something like ""Cannot debug because an existing debug session was found"" in past couple weeks, I assume because of this.",1
1095,Please resolve this issue ... this has been a recurring one for every release.,0
1096,"Based on what you've said so far, it looks like the TF processes are not being dying and the workaround is to find them via `lsof /dev/nvidia2` and to `kill -9` them manually.",2
1097,> I'm facing the same issue downloading from Malaysia.,2
1098,I run it at the start and end of the process.,0
1099,"List of existing theme colors:

* `""notebook.cellEditorBackground""`
* `""notebook.cellToolbarSeparator""`
* `""notebook.focusedEditorBorder""`
* `""notebook.outputContainerBackgroundColor""`
* `""notebook.cellBorderColor""`
* `""notebook.cellInsertionIndicator""`
* `""notebook.cellStatusBarItemHoverBackground""`
* `""notebook.focusedCellBorder""`
* `""notebook.inactiveFocusedCellBorder""`
* `""notebook.selectedCellBackground""`
* `""notebook.selectedCellBorder""`
* `""notebook.symbolHighlightBackground""`
* `""notebookScrollbarSlider.activeBackground""`
* `""notebookScrollbarSlider.background""`
* `""notebookScrollbarSlider.hoverBackground""`
* `""notebookStatusErrorIcon.foreground""`
* `""notebookStatusRunningIcon.foreground""`
* `""notebookStatusSuccessIcon.foreground""`
* ~`""notebook.focusedRowBorder""`~
* ~`""notebook.rowHoverBackground""`~

Questions/confusion/requirements
- [x]  `notebook.cellBorderColor` is applied for output background by default #125459
- [x] Support output border color #125462
- [ ] `notebook.cellToolbarSeparator` should be a generic toolbar vertical separator color
- [ ] naming convention (`notebook.status.",0
1100,"For diagonalizable matrices, this way of doing matrix power may also be faster (for large powers).",1
1101,We only show the inline instruction pointer if the previous stop was on the same line.,0
1102,and why I think it is an important issue to raise for tensorflow at large.,0
1103,"Best,",0
1104,-MSFT will take a look,0
1105,"A context manager like this would shadow legitimate coding bugs where tensors are on the wrong device and author should know about this, it would also make life of library developers harder - they'd need to develop their libraries to work when tensors can unexpectedly switch devices under them.",0
1106,"> Reply to this email directly, view it on GitHub
> < or
> unsubscribe
> <
> .",0
1107,what do think of this?,2
1108,"The sampled_addmm in non-masked semantics should be defined as

> sampled_addmm(s, m1, m2, alpha, beta) := addmm(s, m1, m2, alpha, beta).sparse_mask(mask)

> where mask = (s != 0).",0
1109,with different window sizes) have very different effects on outputs.,0
1110,",

With RTX A2000 you are able to use this GPU ?",0
1111,I was able to repro the issue.,2
1112,I haven't explored it but it looks like there is a Tensorflow SSIM [implementation]( as part of their code base.,0
1113,"However, I think that this solution may not be
perfect for handling data augmentation, where frameworks like PyTorch can
set all seeds for a given worker to handle this.",2
1114,"**Any other info / logs**
The best logs can be found on the conda-forge recipe.",0
1115,Looking forward to the feature.,2
1116,- **What tests are associated with source function/method X?,1
1117,Is there a way to merge the 2 integrated terminals into one integrated terminal so that they split into two?,1
1118,"Bit by polymorphism, looks like.",2
1119,You can achieve this by doing something like:,0
1120,"> 
> Note that in practice in a real pipeline, one would include all the patterns to legalize to HLO, include the STD->HLO, and legalize there as a single pass before running the LHLO buffer alloc.",0
1121,"For the same reason I would rather avoid doing the workspace storage scanning in js-debug, but at least that is slightly less dependent on details like how the pipe name is formed, so I would be a little happier with that solution.",0
1122,Good point about folder-specific settings - i've updated my previous message with extra info about that as well.,0
1123,Another approach for testing what you want is to rephrase the problem using strided tensors only (replace a sparse input with a pair of strided values and a mask tensor).,0
1124,One of the fundamental assumptions in gradcheck numerical path is that the inputs are perturbed in-place (for efficiency).,0
1125,Please let me know if you any questions.,0
1126,?,0
1127,same situation in japan,2
1128,"Even if I set the LD_LIBRARY_PATH to the conda path of torch, the issue still occurs",0
1129,"We need a single `.to(device)` call to switch the input to the device of the model and we are done, regardless if the model has dozens of layers or just one.",0
1130,"Assuming Python creates a TestItem for the file, it can associate `feature_func` with the file's TestItem, such that clicking ▷ next to `feature_func` runs the file tests.",0
1131,"652     config = identifier
--> 653     (cls, cls_config) = class_and_config_for_serialized_keras_object(
    654         config, module_objects, custom_objects, printable_module_name)
    655 

~/tf2/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)
    554   cls = get_registered_object(class_name, custom_objects, module_objects)
    555   if cls is None:
--> 556     raise ValueError(
    557         'Unknown {}: {}.",0
1132,Please use some physical devices.,0
1133,They’re kinda different issues and this has been open for 2 years now so I’d rather leave this open until it gets fixed,0
1134,"So we would need to go through the whole cycle to avoid any silent breakage for the users:
- Adding the new reduction as an opt-in and deprecate the current default, asking users to specify a reduction explicitly
- Remove the default altogether, forcing the users to always specify the reduction
- Add a new default that is the new behavior that we want
So that will take 3+ releases before it happens and will be quite a bit of work.",0
1135,[findall](,0
1136,"you cannot assume that [`lifecycleMainService.onWillShutdown`]( is ever called, the application might crash leaving a stale lock file around (cc  who just implemented a locking solution that avoids these kind of issues for the extension host state process via 

I don't really understand why the existing IPC socket cannot be used to figure out if an instance is running?",1
1137,"Downloaded current (1.85.0) vscode, and updated extensions.",0
1138,We assume here that it is so - that is that all params of that layer are on the same model.,1
1139,"(See 

I'd like us to consider implementing a pad function like that to simplify our padding UX.",0
1140,"the issue author or some other repliers are having different expectations)

what is different is that VSCode can continue the editing process after switching back to VSCode.",0
1141,p.s.,0
1142,"Hi ,

I'll try to find a better machine to run the build and contact again with error trace if any.",1
1143,A context manager could take care of it.,0
1144,Can we mark it as not stale?,2
1145,Skipping registering GPU devices...``,0
1146,Do you mean the computation will happen on device 1 instead of 0?,2
1147,"When I fit with a larger batch size, it runs out of memory.",0
1148,> When is this expected to be resolved?,2
1149,same problem,2
1150,My guess is that would require each pytorch op to do such check and it'd probably be inefficient and ugly.,1
1151,"I can see in the development tips that it should be possible to skip cuda, but I don't know how.",1
1152,Yagger is right,0
1153,"The RPC framework can also be used to move activations between shards, and [Distributed Autograd]( and [Distributed Optimizer]( can be used to run the backward pass and optimizer across a sharded model (with relatively minimal code changes compared to single-node training).",0
1154,"If my default device is 1, and my model and data are both on device 0, could you better explain the internal process of Pytorch if I run the model?",1
1155,i wonder if python has some native string views over utf-8 strings?),1
1156,"In the current state we still look for a `code.lock` file, but this is comparatively minimal, and we do so in parallel to looking for the Chrome lockfiles, so it does not add significant performance or complexity overhead.",0
1157,"While the latter would at least be possible in Python (based on file name matching), for instance, there's no way we would be able to associate code to a specific test.",2
1158,"VSCode is not showing the computed type, it is just showing me the definition.",2
1159,"Thank you for reporting the Apex issue, .",0
1160,recently did some work on a [signficant-other]( extension that implements something like this.,0
1161,`Could not load library libcudnn_cnn_train.so.8.,0
1162,"When I create the model, when using nvidia-smi, I can see that tensorflow takes up nearly all of the memory.",0
1163,There were some CSP warnings as jsma mentioned.,0
1164,Finally it is working normal when generating with a normal model that is not opitmized.,0
1165,The bit that is different from the other reductions is that we are normalizing over the weights (dividing by the sum of the weights).,0
1166,"If not, is there somewhere where we can read up on the ideal build order.",0
1167,"For me most of your use case is solved by using YAML instead of JSON (like Sublime but it's frustrating that there's a [compile step]( for Code) and the metaprogramming facilities of [embedding match content in scope names]( (using them like CSS classes to inject other grammars) or YAML 1.1 [merge keys](

How YAML looks (syntax highlighting available for embedded regexes):



I've seen at least 2 projects that rolled their own grammar generators (the original Reason syntax and your own Better Shell Syntax).",0
1168,It's very fast most of the time.,0
1169,": Yes/No
Yes
<!-- 🪓 If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause.",0
1170,"/easycla
As part of the transition to the PyTorch Foundation, this project now requires contributions be covered under the new CLA.",0
1171,hlo already depends on std - so there is no additional dependence as well FWIW.,0
1172,:D,0
1173,", feel free to take this task if you would like!",0
1174,Many thanks.,0
1175,"In the meantime, the `RangeHighlightDecorations` from [here]( probably diverged.",1
1176,"I realize that perhaps my vision is not quite feasible at the moment, but perhaps there are some ways that can ease this process.",2
1177,But we encounter some race conditions with it.,0
1178,I really appreciate any help you can provide.,1
1179,"**Workaround works with h5 format**
h5 format saving works, but cannot load the model

`
loaded_model = tf.keras.models.load_model('/tmp/model/probabilistic_model.h5')

`
Error while using h5 format for saving and then loading the model is shown below.",0
1180,(Python 3.11),0
1181,FWIW that should be possible but would involve measuring each font to check that they are indeed monospace (`ctx.measureText` for i and w?,2
1182,[ti 05 april12:02:57+02002022](,0
1183,"Yeah, customizable (n-)click event behavior would be fantastic.",0
1184,This did the trick for me: [,0
1185,It so turned out that I ended up being sick the latter part of last week.,0
1186,The API level I'm using should be 33.,2
1187,Safari (`Version 15.4 (17613.1.17.1.6)`) fails on first load for me but worked on refresh.,0
1188,I don't think this is the right behavior.,2
1189,Full example:,0
1190,What's particularly worrisome is that the above behavior is not expected by a user.,2
1191,"The most simple approach that will always work is to just always add: `to(self.get_device)` to all input arguments in `forward`, e.g.",0
1192,"However, I think `reduction='unweighted_mean'` might be misleading too - that indicates to me that `weight` is left out entirely from the mean calculation.",0
1193,"script:

<pre>python3 Remasterer.py
Epoch 1/10
469/469 [==============================] - 2s 2ms/step - loss: 0.3517 - accuracy: 0.8991 - val_loss: 0.2061 - val_accuracy: 0.9383
Epoch 2/10
469/469 [==============================] - 1s 2ms/step - loss: 0.1788 - accuracy: 0.9480 - val_loss: 0.1503 - val_accuracy: 0.9539
Epoch 3/10
469/469 [==============================] - 1s 2ms/step - loss: 0.1298 - accuracy: 0.9612 - val_loss: 0.1310 - val_accuracy: 0.9601
Epoch 4/10
469/469 [==============================] - 1s 2ms/step - loss: 0.1019 - accuracy: 0.9692 - val_loss: 0.1030 - val_accuracy: 0.9688
Epoch 5/10
469/469 [==============================] - 1s 2ms/step - loss: 0.0848 - accuracy: 0.9735 - val_loss: 0.1042 - val_accuracy: 0.9677
Epoch 6/10
469/469 [==============================] - 1s 2ms/step - loss: 0.0719 - accuracy: 0.9776 - val_loss: 0.1011 - val_accuracy: 0.9683
Epoch 7/10
469/469 [==============================] - 1s 2ms/step - loss: 0.0612 - accuracy: 0.9809 - val_loss: 0.0838 - val_accuracy: 0.9742
Epoch 8/10
469/469 [==============================] - 1s 2ms/step - loss: 0.0528 - accuracy: 0.9836 - val_loss: 0.0915 - val_accuracy: 0.9711
Epoch 9/10
469/469 [==============================] - 1s 2ms/step - loss: 0.0459 - accuracy: 0.9857 - val_loss: 0.0978 - val_accuracy: 0.9689
Epoch 10/10
469/469 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9869 - val_loss: 0.0906 - val_accuracy: 0.9719
313/313 [==============================] - 0s 685us/step - loss: 0.0906 - accuracy: 0.9719
Test accuracy: 0.9718999862670898
WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.",0
1194,":


this would work without needing multiple `if MP: ....` which some of the current MP implementations use with a slight overhead for the non-MP case.",0
1195,"I'm trying to keep the issue count down, there are several issues relating to this and this is how I see them:

-  Pull tabs out into floating windows - This is a blocker to what you're after
-  Tabs for terminal - This would likely be needed to pull *individual* terminals out, not the whole panel (not sure if there's an issue for pulling the panel out or not)

Once both of those are implemented you would naturally get the functionality you're after for free.",2
1196,I request you to please follow the instructions as per official [documentation]( and then let us know if still having same problem.,2
1197,You may have to add `c10::complex` to the table.,1
1198,"Sneakily moving tensors across devices would be a source of bugs, performance cliffs, and user confusion.",0
1199,The files are being displayed and saved correctly.,0
1200,"Back to you captain, this sounds to me like a service worker issue or issue with how we setup vscode.dev.",1
1201,And it still failed with the quotations removed from CUDA_VISIBLE_DEVICES and TF_CPP_MIN_LOG_LEVEL.,2
1202,"I had checked the guide to reduce binary size before, the thing is my laptop is freezing in the middle of the build with bazel when I set SELECT_TF_OPS, and that's why I wanted to reduce the number of ops included.",2
1203,"Haven't tried building from source yet, but it will take too much time, which is not an ideal solution...

Just want to double check why this is happening?",2
1204,I have received some negative feedback on the whole approach e.g.,0
1205,But messing around with the dist.set_debug_level() has me kind of confused on how to use this.,1
1206,After a week of struggling with microsoft/vscode-textmate#32 and practically nonexistent documentation apart from [a blog post]( from 2014 ... could we pretty please with a cherry on top have an update on this issue?,2
1207,You are seeing some results from files that you have open because there we just search in the open buffer.,0
1208,Probably NumPy / HDF5 / Apache Arrow / parquet / data frame libraries have also some support along these lines.,0
1209,"FYI, I don't actually have any edits to make.",0
1210,> > Do we agree that gradcheck producing True with masked=False is incorrect?,2
1211,What I meant is if you infinitely scroll in the disassembly view then step next it's pretty much to re-position yourself as you've scrolled hundreds of thousands of lines aware from where you were before.,0
1212,"Hi -arm,  yeah that documentation is incorrect, we have not finalized on intended behavior so we have not updated it to prevent further confusion.",0
1213,"> With latest version tf-estimator was deprecated and no longer needed for build and refer the [source](

I'm not sure I understand.",0
1214,pip?,2
1215,> Is densify documented?,1
1216,"> Going further, maybe some simplistic dataframe class can be added to PyTorch (being a tuple of tensors with having equal leftmost dim).",1
1217,same in ubuntu1804 x64 libtorch1.3 cuda10.1,0
1218,"As you can see at the end of video, I'm unable to scroll anymore, nor does the actual scrollbar show.",0
1219,As ML preproc are in general not as complicated as analytic DB workload.,0
1220,"I decided to use an AWS EC2 instance with Docker to avoid possible dependency conflicts, but still the build fails.",1
1221,Cc,0
1222,"Using valgrind to debug:

### Environment

- Ubuntu 14.04
- GCC 5.4.0 , CMake 3.12
- CUDA 9.0.176 , CUDNN 7.0.5
- PyTorch 1.0.1 , libtorch 1.0.1 stable
- CUDA used to build PyTorch: 9.0.176
- python 3.6.6",0
1223,"But this is not always the case, as for example shown in the trivial example of 

4.",0
1224,you are right.,0
1225,There should be a way to cleanly clear the RAM from Tensorflow,0
1226,I see what you mean by the race condition.,0
1227,There are multiple issues being discussed.,2
1228,I saw in the survey responses that several had complained that it's difficult to see where code cells are and where markdown cells are.,0
1229,"The cuda implementation was just to get a faster implementation, since I use matrix exponentials quite a bit myself.",0
1230,Yeah.,0
1231,* There are discussions/exploration about whether some lightweight structure over Tensor could work.,0
1232,I think #40829 should be re-opened as it makes sense it's a different issue from this one.,2
1233,"<a href="" src="" alt=""CLA Not Signed"" align=""left"" height=""28"" width=""328""></a><br/><br /><ul><li><a href=' target='_blank'>:x:</a> - login:  / name: Ivan Yashchuk .",0
1234,Therefore I don't need it.,0
1235,"Traceback (most recent call last):
  File &quot;/home/dbts2023/Desktop/venv/Remasterer.py&quot;, line 171, in &lt;module&gt;
    gan.add(discriminator)
  File &quot;/root/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/trackable/base.py&quot;, line 205, in _method_wrapper
    result = method(self, *args, **kwargs)
  File &quot;/root/anaconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py&quot;, line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;/root/anaconda3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py&quot;, line 280, in assert_input_compatibility
    raise ValueError(
ValueError: Exception encountered when calling layer &quot;sequential_4&quot; (type Sequential).",0
1236,Hi yes the behaviour is the same for all versions above 2.12,0
1237,"Ah sure, sorry I am not that familiar with gbd.",1
1238,"Hi, all!",0
1239,<!-- ⚠️⚠️ Do Not Delete This!,0
1240,Any insights or solutions would be greatly appreciated.,0
1241,Has anything changed in the current state of VSCode that would make it easier to implement fuzzy search everywhere?,1
1242,"But, for some reason I run into this when I have only onnxruntime-directml installed though.",1
1243,`densify` is not needed at all.,0
1244,"iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0rc2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](
You can also obtain the TensorFlow version with:
1.",0
1245,"Editor note: This is #7065

----

> I've got the same issue under macOS 10.13.6.",2
1246,Figuring out ways to remove those UI components' services.,1
1247,"Would very much appreciate this feature being added; currently using torchvision implementation as a workaround:

As noted in the originally referenced paper, Replication padding is also equivalent to symmetric if padding is 1.",0
1248,The development of keras has been moved to keras-team/keras repo.,0
1249,py_function requires GIL and is generally not recommended to be used in tf.data except for experimental purpose.,0
1250,"It can be build in parallel with any other TF process, it doesn't need TF afaik.",0
1251,It would be pretty easy to enable fuzzy filtering everywhere (ranking results naively on the order of appearance) while still having a custom ranking logic for file paths.,0
1252,Thank you  removing it from 2.2.2 milestone.,0
1253,"But in that case we could consider to support passing an opaque option bag via the ""runInTerminal"" request.",0
1254,I don't think it's documented too well.,2
1255,Could you please check it out please?,2
1256,"For this reason, we would like to activate this option to be sure the port will be reserved to tensorflow.",0
1257,"Sincerely,
Vladimir Zaigrajew
śr., 4 sty 2023 o 12:48 SuryanarayanaY ***@***.",0
1258,Can you please go ahead and close this issue as its already been discussed [here]( Thanks!,0
1259,Even then the current/match-line background is too subtle for my eyes/monitor.,0
1260,Hi all!,0
1261,"Affects `torchvision`, `torchaudio`, `pytorch` for me.",0
1262,hmm i don’t think this exists in pytorch tho.,2
1263,"Failure after conversion
Model produces correct results on my PC, but an error occurs when tyring to load it on an Android device.",2
1264,When do I get this feature in Visual Studio Studio Code?,1
1265,"**
No

**Who will benefit with this feature?",2
1266,"(Aside, on Windows stating a pipe exists will cause a connection to be made to that pipe, not sure if that would cause issues.",1
1267,I am wondering if there has been any progress.,1
1268,> what would be most valuable would be to show the test play button in the gutter by the related code.,0
1269,but this is still a workaround to this feature request.,0
1270,"In China, tensorflow is dramatically used in recommender system.",0
1271,"To reproduce this, please follow the steps below:
1.",0
1272,"Not at this time, sorry .",0
1273,"> By the way, what are you looking to log?",1
1274,can you run something like,2
1275,For Tensorflow to detect GPU we should follow the instructions for GPU setup as mentioned in documentation.,0
1276,"We have no problem in Eclipse, there's no problem in writing similar things there, they have no issue with things running in the render thread.",0
1277,"Thanks for sharing the blog post as well, but I am not sure I follow the argument there.",2
1278,Sorry to spam you but would a separate PR proposal for #50140 be more productive 🙏🏼,0
1279,This makes it really hard to develop when the types are complex.,0
1280,"When you run the code in the above Colab notebook, a TFLite model named `model_fixed_batch.tflite` will be generated.",0
1281,"If you want the return value, then maybe you need
> to write it to some global data structure.",0
1282,"Better alternatives:
4.",0
1283,Reminds  me that I should do column BPs next month.,0
1284,Might be helpful to also offer them as downloads from,0
1285,"it would be perfect if we can set a custom regex match or tell the cursor to highlight only characters and exclude leading/trailing whitespace on triple click, for example -- iterm2 has this feature at Settings -> General -> Selection -> Double-click performs smart selection.",0
1286,"Hi , thanks for taking a look at this issue.",0
1287,"As a last resort, I will try the solution proposed by  to spawn a subprocess every time I need to switch models and I will report on how it goes.",1
1288,Neither `masked=True` nor `masked=False` allows me to do that.,0
1289,"---

Edit from :

There is some confusion about the scope of this issue, here is the clarification as the thread is long:

- This issue is tracking making the terminal ""stand alone"", ie.",0
1290,If it reports 4 when it should be 7 then training isn't going to work properly when it saves and resumes.,0
1291,e.g.,0
1292,"So in order to create a `symmetric` type argument, would I have to change/add some code to `padding.h`?",1
1293,"Then we can land the docs version of the PR quickly, and we'll only have to worry about the deprecation in the code",0
1294,I am curious why you want to start the servers upfront?,1
1295,Now I understand from 's comments that I need to add torch.cuda.set_device() to make it efficient.,2
1296,"Oh, great, I was actually thinking of experimenting with fairscale's PP first, since we have just integrated their Sharded DDP into the HF trainer.",0
1297,).,0
1298,It would be nice if matrix_power also supported non-integer powers like 0.5 and other cases.,0
1299,This API is actually really good -the implementation that Huggingface uses for this functionality is incredibly scuffed by comparison.,0
1300,Please refer that defect.,0
1301,I'm glad this is being worked on.,0
1302,"Does the shuffle function use
this seed, or does it set a global seed every time?",1
1303,> I had the same problem.,2
1304,When using extensions that draw decorations this issue effectively leads to search results never being highlighted.,0
1305,(See,0
1306,This is the problem with `densify` as it actually re-parametrizes functions in an unexpected way to cover the whole space.,0
1307,That is the issue I would like you to see.,2
1308,"> An open question is how to handle ""one to many"" situations in both directions.",1
1309,",

Can you please refer to similar issues #44751,#47782, #46044 for installation and let us know if it help?",2
1310,"While ""better"" in some sense, this is not fun to configure.",0
1311,"I run VSCode Insiders, which updates frequently, so I wrote and shared the above auto-patch shell script.",0
1312,> But it would be better to try to get rid of that and figure out how to hide the search decoration only when the cursor moves.,0
1313,I got the impression that something broke in TF memory management when Keras was integrated into TF.,0
1314,We typically only tolerate this when there is very strong need for it and try to avoid this at all costs (cc ).,0
1315,"> 
> 
>  I think this prints the number of individual batches where the updates have been performed, in your case if you are doing n iterations on your data then `optimizer.iterations` prints n.

The problem is that OptimizerV2.iterations is exactly what gets passed to a LearningRateSchedule to determine the learning rate.",2
1316,"When you set the focus, the editor places a faint top/bottom border on the line (good).",0
1317,"For instance, in Pester you can tag tests with anything.",0
1318,Also why 5 and not 3?,1
1319,Why this issue is closed ?,1
1320,"I'm not sure why you are asking me for a PR when the discussion hints that bootstrapping is just really not that easy as explained in the comment  

I am really not the best person for this since I am merely a user of tensorflow.",1
1321,"I don't actually know how the pytorch gradients are defined (yet), but I could probably dig up/work out the math for the gradient if there's interest for that...~
EDIT: oops actually it's probably not useful in practice... anyway, the SVD + elemwise exp seems good",2
1322,"So, e.g., show me something like



It is probably unwise to actually print the contents of `x_1`, since it might be quite large.",0
1323,"I'm not sure if cublas is failing here, as `gdb` shows the correct path to the pip wheels.",1
1324,"Same here, wondering if they are even working on that..",1
1325,"Please find the [gist](

Thanks.",0
1326,"Any chance you could update Firefox to latest and try again, reloading with devtools opened?",2
1327,"Within non-masked semantics, `f(x)` and `g(x)` are equal because unspecified values in `x` are treated as implicit zeros: `f(x).to_dense() == g(x).to_dense()` for any `x`.",0
1328,Do you have some pointers that describe how to propagate the cluster spec ?,1
1329,Implementing behavior like this would be tricky because a core tenet of PyTorch's UX is that it avoids implicit cross-device data movement.,0
1330,"But I hope it helps narrowing down the issue :)


Edit:
Has the flag been introduced here?",2
1331,"Thanks for the feedback, all.",0
1332,"Ideally, we still need to inspect Jacobian to make sure that ""irrelevant' entries are all zeros...",0
1333,"Okay, got it, DAP is a general protocol which I didn't know much about which I guess allows people to create debug profiles that work with multiple IDEs.",0
1334,"xla_hlo.const -> xla_lhlo.const would work, but the -hlo-legalize-to-lhlo pass could potentially be presented with such an input in other ways, and so making -hlo-to-mlir-hlo generate hlo.const isn't a real solution.",2
1335,"Hi , I was able to install if I downgraded Python to 3.9.17, can you try that out to see if you are able to continue that way?",2
1336,"> Same problem in CUDA 12.2 + CUDNN 8.9.7 + torch 2.2.0
> 
> reinstall torch == 2.1.x solve the problem

Doesn't work for me.",2
1337,Can you reproduce on a separate machine?,1
1338,"Yes, thank you
-Stewart 
> Using Search sidebar to find all items in **current file** requires **too much clicks**.",0
1339,Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU.,0
1340,[Screenshot_20231101-142051_resized](,0
1341,"Please ensure this object is '
    558         'passed to the `custom_objects` argument.",0
1342,What is the status of this?,1
1343,"I also need this functionality, particularly when using Jest for unit testing.",0
1344,"Not being able to reserve a port is really a Yarn issue, not a TF one.",0
1345,> Has there been any progress made on this?,1
1346,"Hi guys,

I am hacking vscode to make the integrated terminal a standalone app.",0
1347,cc         -varma      -Huang,0
1348,Groups seemed the perfect solution but then I don't understand how the view column APIs work (are they for creation of a dynamic grid of terminals like the group concept or just a static grid?,1
1349,"> 
> Lastly, if you insist to have this pattern, it should at least live in a different file.",0
1350,Please post this issue on [keras-team/keras repo.,0
1351,"I have 2 monitors and I want to dock out the terminal, output and etc which dock on the bottom of the editor to be in my another monitor with its own window which give me clearly space to work.",0
1352,"[here](
cc",0
1353,"Same from India, takes ages!",2
1354,I think it would be great if there were a properly tested SSIM and MS-SSIM in Pytorch.,0
1355,"> Hi 
> 
> Could you please try again as I was successfully able to install mediapipe model maker on ubuntu(colab).",2
1356,"In the generator, as soon as I comment out the Masking layer, the warning disappears.",0
1357,"So, I don't see the argument that the underlying problem is that of a memref constant store/assignment/filling in the std dialect.",1
1358,Here's another example.,0
1359,The workbench is usually better equipped in knowing when to ask for what.,0
1360,"With master branch as of 831a55584749593400807e0baa7478476b5dbc70 (May 26):
The xla hlo to lhlo lowering doesn't convert completely when the operand of an op (in this example below, that of broadcast_in_dim) is a constant tensor.",0
1361,"**
Add SO_REUSEPORT option when starting tensorflow training server.",0
1362,"The Structural Similarity Index (SSIM) is generally considered to be a milestone in the recent
history of Image Quality Assessment (IQA).",0
1363,"## Additional context

The *""symmetric padding""* is also popular in wavelet transforms or discrete convolutions, see e.g.",0
1364,"A related discussion on importance of reducing GC pressure and number of Python objects:


So native StringArrays + some basic data frames are a useful idea for dataset classes",0
1365,Can you try to reproduce this on you system with TF `2.11.0-rc2`?,2
1366,"Given the nuances with memrefs, such a constant filling op cannot live in std, but only live in a dialect meant for dense tensors/tcp/linalg/lhlo etc.",0
1367,"This involves several parts of fix like:
   1. some fix in python layer as only variables are trainable now.",0
1368,"98     objects = save_impl.wrap_layer_objects(self.obj, serialization_cache)
---> 99     functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
    100     # Attribute validator requires that the default save signature is added to
    101     # function dict, even if the value is None.",0
1369,I _could_ but I'm not seeing evidence this is browser specific.,1
1370,"In this case, I'm not sure what else is there to try.",1
1371,I have moved it to be under `gs://tensorflow/versions/2.16.1`.,0
1372,One possibility is using the same API they have to implement column BPs.,0
1373,> When I upgrade torch to 2.2.1 then I get several version errors refering to torch-directml incompatability.,2
1374,"For example, see how tensorflow depends on `tensorflow-io-gcs-filesystem`:


And how `tensorflow-io-gcs-filesystem` depends on tensorflow:



Typically, these circular dependencies are broken by having a ""core"" package.",0
1375,"On the Python side there is no way to automatically or easily infer which tests are related to a specific piece of code (no naming conventions), so we would probably not use this feature as it stands right now.",2
1376,"Type a name

Expected result:
Either the window gains focus after right clicking, or the context menu never appears, or the options that cause issues are greyed out, or the text field doesn't disappear once the window gains focus.",2
1377,But I'm not sure anyone on our end is looking into this.,1
1378,When is this expected to be resolved?,2
1379,This issue proposes supporting complex numbers natively within inductor.,0
1380,This issue is stale because it has been open for 7 days with no activity.,0
1381,"However, the latest archive with the `libtensorflow` on the official website [is still 2.15]( Where can I get the latest 2.16.1 `libtensorflow` with GPU support for Linux?",2
1382,"But in the specific case of both files being opened, I very much like to automatically connect the source ranges.",0
1383,I found a reference [here]( and manually removing `/usr/local/cuda/lib64` in `$LD_LIBRARY_PATH` will fix the problem.,0
1384,I understand anything around tokenisation requires a refactor and that’s most likely why no one wants to go near it but how long can that last really?,1
1385,I had a look at using ClusterResolver instead of TF_CONFIG but it seems that this doesn't solve this problem.,2
1386,"Thank you very much for your work on this issue, it seems like there is an understanding as to why this happens, but if there is any way I can help please let me know.",0
1387,I confirm that mediapipe model maker installation fails on Windows with the error mentioned above.,0
1388,"But `f(x)` and `g(x):=f(densify(x)` are different functions, right?",1
1389,"Such degradation of the function definition means that `gradcheck(torch.sparse.sampled_addmm, x, ..., masked=False)` is not testing `torch.sparse.sampled_addmm` autograd-correctness, but of the `torch.addmm` equivalent.",0
1390,I haven't studied this approach yet.,1
1391,You could even have a version that allocates the buffer if you wanted.,0
1392,I would then prefer having std.const to lhlo.const in the hlo to lhlo conversion which was the original proposal.,0
1393,> In this case the backward formula for `self` should take `alpha * (m1 @ m2).sparse_mask(self)` into consideration..,0
1394,", I trust I can figure out the new API, but if there are existing examples/tutorials that always helps to save time.",1
1395,thx,1
1396,"So, std.constant -> lhlo.const is indeed a lowering that is complementary to have within hlo to lhlo.",0
1397,Hi any updates here?,1
1398,Maybe a straight-forward way to tackle this would be to move `searchModel` to `/browser/`.,1
1399,:-),0
1400,"* `'weight_normalized_mean'`: It is just the weighted mean, but as folks before me pointed out, that is still confusing since all the other reductions include a weighted sum.",0
1401,It is used in specific language files where enconding just can't be changed no matter what.,0
1402,Though what I am not sure about is why the behaviour in search is different when searching for different things.,1
1403,"Additionally, the [libtensorflow nightly archive]( also has not been updated for about sixty days.",0
1404,The GCS filesystem is built by SIG IO community.,0
1405,"My Cuda driver is version 12, but CuDNN is indeed version 8.9.0.",0
1406,"> we wouldn't want a stand alone terminal that couldn't show more than one instance at once

Even as a proof of concept, tmux would make this already very useful for me!",0
1407,Upvote.,0
1408,"It just increases the risk of race if the file is written and the process is shut down before the lifecycle handler is registered, but this risk is present regardless.",0
1409,Can you reproduce if you disable them all?,1
1410,"I had _""search.useRipgrep"": false,_ - commented that out
> 3. and under [myspecificlanguage] add that _""files.encoding"": ""maccenteuro""_
> 
> multi-file search works, open file search works, file is still in _maccenteuro_ and special characters didn't break
> other language file searching (like .js for example) isn't affected
> 
> --
> Make sure you don't have extra .vscode/settings.json in your source code folder.",0
1411,">    
>    PiperOrigin-RevId: 442029734
>
> tensorflow/python/eager/context.py | 3 ++-
> 1 file changed, 2 insertions(+), 1 deletion(-)

I'm not sure about what's activated with this flag that could lead to the error I am seeing.",0
1412,"The future is JAX/Pytorch, TF is doomed to be a relic of the past at this rate.",0
1413,Should I keep  going or let you take over?,0
1414,This would provide a reproducer for the degradation defect in `gradcheck` for input functions that numerical jacobians may be sensitive to the `densify` usage approach.,0
1415,"In this case the backward formula for `self` should take `alpha * (m1 @ m2).sparse_mask(self)` into consideration, so it is expected for `gradcheck` with `masked=False` to fail.",0
1416,"So, this is definitely not suited for time-critical operations.",0
1417,"See more at 
2023-05-23 14:23:21.673109: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries.",0
1418,I would not be the right reviewer of this.,0
1419,"> 
> My conservative guess would be by end of next week.",0
1420,"I'd say you probably shouldnt have onnxruntime and onxnruntime-directml installed at the same time, but I am not 100% sure about it.",1
1421,"If we were to support the post-accumulate-grad hook, then you should be able to maintain your own online $\ell_2$-norm squared as you process gradients through backward and then compute a square root at the end of backward to get your overall gradient norm.",0
1422,I.e.,0
1423,Setting the `TF_RUN_EAGER_OP_AS_FUNCTION` to `false` helps and solves the issue for TF 2.10.,0
1424,First I need to get  out of the draft mode.,0
1425,I'm just writing here for visibility.,0
1426,"(This app is a slightly modified version of the StyleTransfer app from tensorflow/examples, designed to reproduce the error.)",0
1427,Ok then why has it failed with me even though I have done the exact same tests?,1
1428,Add pointer glyphs in left and/or right gutters.,0
1429,"In particular, what should be the output shape?",1
1430,Please check if library `libnvidia-fatbinaryloader.so.396.44` is present on the system and if its path is on LD_LIBRARY_PATH.,1
1431,It sounds like `highlight all selections` to me but I'm not quite sure.,0
1432,"Hi , I'm looking into this but in the mean time you might want to check if you do any broadcasting in your model, the GPU delegate generally does not handle this case very well currently ex:  .",2
1433,"I cannot figure out why here 1 matchs but 2 not ? 

!",1
1434,"When trying to implement this logic in the node (legacy) DA, it became clear that we would have to implement another heuristic (now in the backend) because node would just not provide the information whether a line has one or more statement locations (and needs to return a column attribute or not).",0
1435,"Sorry, what is the fundamental assumption of gradcheck?",1
1436,"When I upgraded to 2.9.1 or 2.10.0 or 2.11.0 versions , I am seeing issue with memory usage and performance.",2
1437,would it be possible to post the log for the failing run created by `LD_DEBUG=libs python script.py args`?,2
1438,I'm trying to use VSCode with Unreal Engine installed on an external SSD and your patch breaks my setup.,2
1439,"To know more see this TF forum discussion ; 
[
Thank you!",0
1440,what about this one?,1
1441,"If we provide such a robust function, the user has to be informed about that!",0
1442,So how would that be supported in FSDP?,1
1443,"Agree, that from the user perspective this is not so obvious so let's keep it open as a feature request.",0
1444,With the current tf behavior  we are blocked because we need to free the ports all the time (before  starting the tf server) and then we don't even know when tf assigns them again.,2
1445,I think the right direction is to move toward supporting register_post_accumulate_grad_hook so that the user can customize as desired.,0
1446,"* add a new API like embedding_lookup_sparse, or do modification on it.",0
1447,"Build `estimator`....
3.",0
1448,"For consistency and completeness, I think we definitely should.",0
1449,"People, please put ""thumbs up"" on this issue, so VS Code team can see it's important!",0
1450,I see  mentioned  I'd like to pick this up!,0
1451,I'll wait then.,0
1452,ok I will see what I can do if have some time.,0
1453,"I tried  ""TF_RUN_EAGER_OP_AS_FUNCTION"" in 2.9.1 /2.10.0/2.11.0 but still the memory and performance issue persists.",1
1454,Thanks for writing this up.,0
1455,"However, I couldn't get this to work, despite the string between `tasks.json` and `launch.json`.",1
1456,"However, the only way I can then release the GPU memory is to restart my computer.",0
1457,"The facility made it least _possible_ to write our language mode in Monarch, even if it was _much_ harder work than doing it the low level way, which is what we ended up doing.",0
1458,Until [competition]( begins to narrow?,2
1459,Uninstalled and reinstalled the cuDNN libraries but same error.,2
1460,`sampled_addmm` does satisfy `func(densify(x)).to_dense() == func(x).to_dense()` holds for `x`.,0
1461,"Editor note: This is #17139

----

Thank you, .",0
1462,"The numerical path in `gradcheck(func, x, masked=False)` is equivalent to the path in `gradcheck(func, densify(x), masked=False)`, that is, before computing numerical jacobian, all sparse inputs are mapped to sparse inputs that don't have unspecified elements (these are materialized to zero values).",0
1463,",
Could you please check [this]( Tensorflow documentation and let us know if it helps.",2
1464,"Same from France, using 2 different internet operators (ISP)",2
1465,"For something like this, you don't need to build PyTorch from source, you can use tools/nightly.py to get working binaries for your dev copy.",0
1466,"---------------------

You mentioned the default device several times.",0
1467,This seems to be getting dormant from the core team.,1
1468,"Thanks.

!",0
1469,For performance reasons we want the tokenizers to run in the render process.,0
1470,"## 📚 Documentation

 writes:

-----------------------------------------------------

I wrote this tutorial —  — for steps to add a new scalar/tensor type.",0
1471,"Cool, looks like I do not have any major holes in my understanding of the current state of PyTorch internals then.",1
1472,"Currently when I triple-click a word, the whole line gets selected.",0
1473,IMO what would be most valuable would be to show the test play button in the gutter by the related code.,0
1474,Very excited about this!,0
1475,Needs to contrast well with syntax coloring.,0
1476,These two concepts are different and complementary).,0
1477,>  can you try building from source and see if that resolves the problem?,1
1478,I’d probably go as far to say I’d happily wave any new feature for a few months if it meant time was spent on this.,0
1479,"Note that the current PyTorch implementation for class weights is also problematic when expanding `CrossEntropyLoss` to support ""soft"" labels (target class probabilities); attempted in #61044.",0
1480,"I had the same issue, a fresh install solved it (removing the pytorch directory and doing the install steps from scratch).",2
1481,It looks like there are quite a couple of supporting arguments to improve the current state of searching.,1
1482,Bold the text of the line.,0
1483,"Thanks for the update,sure we will do that",0
1484,"To cover this case as well as avoid breaking BC, I propose we add a new reduction mode that implements the TF-like semantics.",0
1485,I have published a npm package for `Expand` and `ExpandDeep` based on the SO answer:,0
1486,This bug is a nightmare for notebook users.,0
1487,From the debug logs:,0
1488,Ok.,0
1489,"No, I don't think the inspector protocol has anything to help here.",2
1490,"These functions are different because operate over different masks, and mask is a hidden parameter.",0
1491,"Also, since it's an error during model loading, the input data size should not be an issue here.",2
1492,[snip_20161103091454](,0
1493,"because every time I try to use the SSIM, I'm facing with that error.",2
1494,"> For my specific needs, it would be very helpful if we could provide a scalar data type factory api, using which the basic setup to add a new tensor type could be done while still keeping all the heavy-lifting in the c++ extension.",0
1495,"Search on: `defaultMaximumTruncationLength =`
3.",0
1496,The difference is that highlight only shows occurrences that are on the current screen while in VS I see all occurences condenced on the bottom pane (similar to the Search sidebar in VS Code).,0
1497,"There's an [extension]( that does exactly this, but it's half-arsed and stalls the editor.",0
1498,Expanding selection to brackets/braces/quotation marks via triple click would be amazing improvement to copy-paste routines.,0
1499,On light (if I'm remembering correctly) it just passes the color contrast ratio.,0
1500,"Create some fake fake data, labels and weights
2.",0
1501,This may be related to `py_function`.,1
1502,"-roy 

Thanks for chiming in.",0
1503,We cannot just change the default as it would potentially change a lot of user-code in quite un-expected ways.,1
1504,"VS has a find all option

- VSCode Version: 1.6.1
- OS Version: Windows Server 2008

Steps to Reproduce:

1.",0
1505,Can we reopen the issue please?,2
1506,"Was able to replicate the issue in TF v2.7.0,please find the gist [here]( !",2
1507,"Hi , do you have any updates on this?",1
1508,"Concretely, for the Cross Entropy loss, the PyTorch way of doing it is

!",0
1509,"You have stripped down the repo a lot, already yeah?",2
1510,You will need to remove f-strings and test if this works on multi-GPU systems.,0
1511,If this PR is blocked on the XLA lowering ( perhaps someone from the XLA could implement that part?,2
1512,Then you could skip all of that goop.,0
1513,"](
[Or download .vsix package from GitHub page and install it manually.",0
1514,"I retried as suggested, no `--config=monolithic` option and using `--confing=android_arm64` as target.",0
1515,I don't think the current behavior is preferred for the imbalanced class case either.,2
1516,To make it version independent you could also just check for `.sock` files.,0
1517,This whole idea of not letting std dialect ops mix with lhlo would only hurt transformations and conversions out of the dialect.,0
1518,"A generalization based on SVD ( may also be useful (for simple graph signal processing): diagonalize the matrix somehow (svd by default, maybe some optimized svd for symmetric matrices), apply the user-supplied function to eigenvalues, re-multiply the factors back.",2
1519,* dealing with files as a means to lock a folder is complicated.,0
1520,"Yes, I figured by now there is no solution.",2
1521,Please post this issue on [keras-team/keras]( repo.,0
1522,Anything that allows a compound launch config to launch integrated terminals in side-by-side clusters to group related terminals visually would do the job.,0
1523,"Not in any Node version, though.",0
1524,Hope it helps.,0
1525,> Being able to undock the terminal panel as it’s own maximizable window and switch from dropdown to tabs should allievate a lot of pain.,0
1526,Most image-to-image translation tasks would benefit from this.,0
1527,good point!,0
1528,"> These functions are different because operate over different masks, and mask is a hidden parameter.",0
1529,Incognito mode worked right away.,0
1530,"I can't seem to either pull or checkout, It gives me a FileNotFoundError.",2
1531,"Another useful concept can be ""string lists"" that allow appends (with some exponential storage reallocation): 

Related issues on ""zero-copy"":    (about getting a `bytes` view over a sub-tensor - can be useful as an ascii string substitute, and in general for zero-copy pytorch interop.",0
1532,"## Motivation

This is prompted by the paper [Mind the Pad -- CNNs can Develop Blind Spots]( (Alsallakh et al, facebook AI), which investigated the behaviour of different padding methods and how some of them have a detrimental effect to the network.",0
1533,The inability to clear the GPU RAM after clearing the session in production is a huge problem.,0
1534,I think its possible to have a Tree Sitter Service which can emit tokens (similar to the textmate service) and have higher-up services use that instead.,2
1535,I use Windows-1252 encoding in some projects.,0
1536,This issue has been automatically marked as stale because it has no recent activity.,0
1537,It's cleared when any decoration on the model changes.,0
1538,Do you know of an example to study that implements MP with RPC?,1
1539,Firefox 95.0.1 (64-bit) on Ubuntu,0
1540,"If the panel is not maximized, then scrollbar jumpts to top when toggled, which I think is incorrect as well (should remain in the same spot as I left it) but it also somehow breaks: I'm unable to scroll to whole bottom of the terminal, unless I again add new line by pressing enter.",0
1541,<a href=' target='_blank'>Please click here to be authorized</a>.,0
1542,I would very much appreciate some feedback on it.,2
1543,"Another reason to prefer unweighted mean reduction is that (I imagine) people expect the mini-batch mean to be an unbiased estimated of the full gradient*, which the weighted mean reduction breaks.",0
1544,"we have a heuristic when we should try to match on camel casing (by jumping over words), but this heuristic is limited to entries < 60 characters length for performance reasons.",0
1545,"> 
> 
> 
> My understanding is that applying optimizer in backward is algorithmically incompatible with gradient clipping, where there is a dependency on the total norm computed over all gradients before running the optimizer step.",2
1546,Any error in devtools?,1
1547,"But the rescaling by the weights ensures that the loss value for the important sample (assuming some weight are significantly larger than others) remains in a reasonable range, while samples with smaller weights gets much smaller values.",0
1548,The maximized panel could be playing a role in the repro too?,1
1549,I confirmed that at least the issues occurs with the two phones I've tried (Pixel 7 Pro and Galaxy S21 Exynos).,1
1550,"Note that, as a workaround, a combination of `reduction='sum'` with a manual division by N should work.",0
1551,"Plus, I have to use a virtual environment to build the same nvidia-cudnn-cu11==8.6.0.163 package as you, but an earlier version was installed, 8.5.0 in my case.",0
1552,The users should be informed that it is their responsibility to handle domains/co-domains carefully.,0
1553,"[screenshot_20190208_122607](
Issue Type: <b>Bug</b>

Set worskpace encoding to cp437.",0
1554,I guess it should have most of the operator overloads needed for codegen.,2
1555,This is very true and it’s actually quite sad to see too.,0
1556,still super slow from Germany.,0
1557,what about the GPU version?,1
1558,"Hyper is migrating to xterm.js build by , which means that Hyper most likely will be similar to the VS Code console.",0
1559,"### Versions

Current master.",0
1560,> So how would that be supported in FSDP?,1
1561,Split APIs for the terminal will likely look more like the view column APIs.,0
1562,You may try [limiting gpu memory growth]( in this case.,2
1563,"Yes, of course, here is the use case of the PR I have been working on

Here is an example of a `sshleifer/distilbart-xsum-6-6` `BartForConditionalGeneration` model:


Note that I collapsed the huge bulk of it and it's represented by just 2 lines that I wrote myself - it was not the output of the model dump.",0
1564,"You have to reinstall it by compiling from source; or, if you have only one GPU, disable the MPI config in compilation.",0
1565,"Another case of trunk flakiness has been found [here](
            Reopening the issue to disable.",0
1566,Were you able to solve this?,1
1567,Can you please change the mock debug so it won't show negative addresses?,0
1568,"The matrix input is added to the final result.""",0
1569,"Version: 1.63.2
Commit: 899d46d82c4c95423fb7e10e68eba52050e30ba3
Date: 2021-12-15T09:38:17.605Z
Electron: 13.5.2
Chromium: 91.0.4472.164
Node.js: 14.16.0
V8: 9.1.269.39-electron.0
OS: Darwin arm64 20.6.0

Thanks for working on it",0
1570,I agree that something like a context manager would be an interesting UX to consider to address this.,0
1571,"EDIT: looks like this is a dupe of #42749, I'll leave this up for now in case since that issue does not have as reproducible high level example, but feel free to close.",2
1572,Creating new packages that break the circular dependency between `tensorflow` and `tensorflow-io`.,0
1573,And is this only on POSIX or Windows too (I think windows has this notation reserved for UNC paths).,0
